{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "ea1134c8-7cef-468b-a6e1-1ec489df9362",
      "cell_type": "markdown",
      "source": "### Importing Necessary Libraries\r   \nThe required libraries for managing, manipulating, and visualizing data are imported in this part\n\n   NumPy: (Numpy, 2024)    \n   Pandas: (Pandas, 2018)  \n   Seaborn: (seaborn, 2012)  \n   Matplotlib: (Matplotlib, 2012)  \n   CSV (Python module): (Python Software Foundation, 2020)  \n   Regular Expressions (re module): (Python, 2009)\n.",
      "metadata": {}
    },
    {
      "id": "f88f853c-8f16-4aae-a7ef-de808103a011",
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport csv\nimport re\nimport os",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'ModuleNotFoundError'>",
          "evalue": "No module named 'seaborn'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "fd493f8f-cf50-4df4-b372-7273409722a1",
      "cell_type": "code",
      "source": "# Read the CSV file containing quarterly waste generation data into a Pandas DataFrame\nwgen=pd.read_csv(\"quarterly_waste_generation.csv\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'FileNotFoundError'>",
          "evalue": "[Errno 44] No such file or directory: 'quarterly_waste_generation.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read the CSV file containing quarterly waste generation data into a Pandas DataFrame\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wgen\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquarterly_waste_generation.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 44] No such file or directory: 'quarterly_waste_generation.csv'"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 2
    },
    {
      "id": "3eaf16f0-9af7-4b85-a455-048976c4c000",
      "cell_type": "code",
      "source": "# Read the CSV file containing quarterly waste treatment data into a Pandas DataFrame\nwtreat=pd.read_csv(\"quarterly_waste_treatment.csv\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'FileNotFoundError'>",
          "evalue": "[Errno 44] No such file or directory: 'quarterly_waste_treatment.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read the CSV file containing quarterly waste treatment data into a Pandas DataFrame\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m wtreat\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquarterly_waste_treatment.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 44] No such file or directory: 'quarterly_waste_treatment.csv'"
          ],
          "output_type": "error"
        }
      ],
      "execution_count": 3
    },
    {
      "id": "abd0777f-9ca1-48aa-ab20-7b6128ea3c0f",
      "cell_type": "markdown",
      "source": "# 1. Dataset Description and Issues",
      "metadata": {}
    },
    {
      "id": "64b6e67d-6f02-4edb-9d0b-f41e8b93843f",
      "cell_type": "markdown",
      "source": "## Dataset Description\nDecided to merges two datasets: **quarterly_waste_generation.csv** and **quarterly_waste_treatment.csv**, using the columns `Quarter`, `County`, `Waste Type`, `Waste Category`, and `Waste Amount (tonnes)` to create a unified dataset.\n(Pandas.merge — Pandas 1.2.3 Documentation)",
      "metadata": {}
    },
    {
      "id": "b84816b3-e088-4576-832e-7894feaf54c6",
      "cell_type": "code",
      "source": "merge_data = pd.merge(wgen, wtreat, on=['Quarter', 'County','Waste Type', 'Waste Category','Waste Amount (tonnes)'], how='outer')\nmerge_data.head(100)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d0135f29-d68b-4939-9a87-52878fe4e94e",
      "cell_type": "markdown",
      "source": "Giving different names to the the columns in the merged datasets for consistency and clarity (e.g., `Waste Amount (tonnes)` to `waste_amount_in_tonnes`)\n\n### New Column Names\n1. **Quarter:** Time period for waste generation/treatment.\n2. **county:** Geographical region in ireland where waste was recorded.\n3. **waste_type:** Classification as Hazardous or Non-Hazardous.\n4. **waste_category:** Specific type of waste (e.g., Chemical Waste, Agricultural Waste).\n5. **waste_amount_in_tonnes:** Quantity of waste generated or treated.\n6. **treatment_method:** Process used for treating waste.\n7. **price_of_treatment:** Cost per tonne for treatment.\n\n(Pandas.DataFrame.rename — Pandas 1.4.2 Documentation, n.d.)  \n(Pandas.DataFrame.nunique — Pandas 1.3.4 Documentation, n.d.)",
      "metadata": {}
    },
    {
      "id": "7a1dca5a-8549-4260-bc62-da2debf73c9f",
      "cell_type": "code",
      "source": "merge_data.rename(columns={'Waste Amount (tonnes)': 'waste_amount_in_tonnes', 'Quarter': 'Quarter', 'County':'county', 'Waste Type' : 'waste_type', 'Waste Category': 'waste_category', 'Treatment Method':'treatment_method','Price of Treatment (€ per tonne)':'price_of_treatment'},inplace=True)\nmerge_data.nunique()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5aab4258-c798-411e-a459-2791e526e41b",
      "cell_type": "code",
      "source": "merge_data.info()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "806f0cfa-c763-4974-9c62-c6dc3aa067ff",
      "cell_type": "markdown",
      "source": "### Observed Issues\n1. **Date Inconsistencies:** The `Quarter` column contains inconsistent date formats.\n2. **Irregular Text Formats:** Variations in capitalization and special characters were present in  `treatment_method` , `county` and `waste_category`.\n3. **Missing Data:** Null values were present in all the columns.\n4. **Potential Duplicates:** There were duplicate rows in the dataset.\n5. **Outliers:** Possible extreme values in `waste_amount_in_tonnes` and`price_of_treatment`",
      "metadata": {}
    },
    {
      "id": "55d8497c-2b90-4af6-afa7-cf6d1b42ad17",
      "cell_type": "markdown",
      "source": "# 2. Data Cleaning",
      "metadata": {}
    },
    {
      "id": "09b03c78-38f9-4c1a-8d23-5e20eb6dca5d",
      "cell_type": "markdown",
      "source": "### A. Standardizing Date Formats\n- A custom function `normalize_date` was implemented to unify date formats in the `Quarter` column.\n  \n- Formats handled include:\n  - `\"YYYY Qn\"` (e.g., \"2004 Q1\")\n  - `\"YYYYQn\"` (e.g., \"2004Q1\")\n  - `\"dd-MM-YYYY\"`(e.g \"01-01-2004\")\n\n(Pandas.to_datetime — Pandas 1.3.4 Documentation, n.d.)  \n(Pandas.NaT — Pandas 2.2.3 Documentation, 2024)  \n(GeeksforGeeks, 2018)",
      "metadata": {}
    },
    {
      "id": "69d055e4-3359-4357-8ee3-d7caffbd05b9",
      "cell_type": "code",
      "source": "# Function to normalize date formats\ndef normalize_date(date_str):\n    if isinstance(date_str, str):\n        date_str = date_str.strip()  # Remove any leading/trailing whitespace\n\n        # Explicitly check for 'dd-Mon-yy' format (e.g., '01-Jan-04')\n        try:\n            return pd.to_datetime(date_str, format='%d-%b-%y', dayfirst=True)\n        except ValueError:\n            pass  # If it fails, continue to other formats\n\n        # Check for 'YYYYQn' format (e.g., '2004Q1')\n        match = re.match(r'(\\d{4})Q([1-4])', date_str)\n        if match:\n            year = int(match.group(1))\n            quarter = int(match.group(2))\n            # Map to start of the quarter\n            if quarter == 1:\n                return pd.to_datetime(f'01/01/{year}', dayfirst=True)\n            elif quarter == 2:\n                return pd.to_datetime(f'01/04/{year}', dayfirst=True)\n            elif quarter == 3:\n                return pd.to_datetime(f'01/07/{year}', dayfirst=True)\n            elif quarter == 4:\n                return pd.to_datetime(f'01/10/{year}', dayfirst=True)\n\n        # Check for 'YYYY Qn' or 'Qn YYYY' formats (e.g., '2004 Q1' or 'Q1 2004')\n        match = re.match(r'(\\d{4})\\s*Q([1-4])|Q([1-4])\\s*(\\d{4})', date_str)\n        if match:\n            year = int(match.group(1) or match.group(4))\n            quarter = int(match.group(2) or match.group(3))\n            # Map to start of the quarter\n            if quarter == 1:\n                return pd.to_datetime(f'01/01/{year}', dayfirst=True)\n            elif quarter == 2:\n                return pd.to_datetime(f'01/04/{year}', dayfirst=True)\n            elif quarter == 3:\n                return pd.to_datetime(f'01/07/{year}', dayfirst=True)\n            elif quarter == 4:\n                return pd.to_datetime(f'01/10/{year}', dayfirst=True)\n\n        # Check for 'Month dd, yyyy' format (e.g., 'January 01, 2004')\n        try:\n            return pd.to_datetime(date_str, format='%B %d, %Y', dayfirst=True)\n        except ValueError:\n            pass\n\n        # Check for 'Month dd yyyy' format (e.g., 'January 01 2004')\n        try:\n            return pd.to_datetime(date_str, format='%B %d %Y', dayfirst=True)\n        except ValueError:\n            pass\n\n        # Check for 'dd-Mon-yyyy' format (e.g., '01-Jan-2004')\n        try:\n            return pd.to_datetime(date_str, format='%d-%b-%Y', dayfirst=True)\n        except ValueError:\n            pass\n\n        # Check for 'dd-mm-yyyy' format (e.g., '01-01-2004')\n        try:\n            return pd.to_datetime(date_str, format='%d-%m-%Y', dayfirst=True)\n        except ValueError:\n            pass\n\n        # Check for 'dd/mm/yyyy' format\n        try:\n            return pd.to_datetime(date_str, format='%d/%m/%Y', dayfirst=True)\n        except ValueError:\n            pass\n\n        # Check for 'mm/dd/yyyy' format\n        try:\n            return pd.to_datetime(date_str, format='%m/%d/%Y', dayfirst=False)\n        except ValueError:\n            pass\n\n        # Fallback for any remaining date strings\n        try:\n            return pd.to_datetime(date_str)\n        except ValueError:\n            return pd.NaT  # Return NaT if all parsing attempts fail\n    else:\n        return pd.NaT  # Return NaT for non-string entries\n\n# Normalize the 'Quarter' column to a single date format\nmerge_data['Quarter'] = merge_data['Quarter'].apply(normalize_date)\n\n# Ensure the 'Quarter' column contains only datetime objects\nmerge_data['Quarter'] = pd.to_datetime(merge_data['Quarter'], errors='coerce')\n\n# Replace specific dates with corresponding quarters\ndef replace_with_quarters(date):\n    if pd.isnull(date):  # Handle NaT values\n        return date\n    elif date.day == 1 and date.month == 1:\n        return f\"Q1 {date.year}\"\n    elif date.day == 1 and date.month == 4:\n        return f\"Q2 {date.year}\"\n    elif date.day == 1 and date.month == 7:\n        return f\"Q3 {date.year}\"\n    elif date.day == 1 and date.month == 10:\n        return f\"Q4 {date.year}\"\n    else:\n        return date.strftime('%d/%m/%Y')  # Retain other dates in 'dd/mm/yyyy' format\n\n# Apply the quarter replacement function\nmerge_data['Quarter'] = merge_data['Quarter'].apply(replace_with_quarters)\n\n# Display the updated DataFrame\nprint(\"\\nNormalized Data:\")\nprint(merge_data.head(10))",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3b80d12e-818f-4fca-8d9c-1102c8983b47",
      "cell_type": "code",
      "source": "merge_data.head(50)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8f4a3c46-da3b-41d3-b65a-4f8995c272c4",
      "cell_type": "markdown",
      "source": "### B. Standardizing Text Fields\n- Removed redundant prefixes like \"county\" and \"co.\" from the `county` column using regex.\n  \n- Capitalized the values in `county`\n  \n- Finding the unique values in `waste_category`, `waste_type` and `county`. The purpose of the \"unique() method\" is to know the range of values especially in categorical columns, identifying missing values if NaN appears in the output and to check if the values in the column don't have any issues (e.g. the wording of the values, unwanted characters like \".\" inbetween each word)\n- Removed extra spaces between words, replaced characters like \".\" and  \"_\"  with  \" \", then adjusted inconsistent capitalization in `waste_category`.\n- Using the mapping method we corrected incomplete and inconsistent entries in the column `waste_category` like \"Waste Agricultural\" to \"Agricultural Waste\".\n\n(Pandas.Series.str.replace — Pandas 2.0.3 Documentation, n.d.) (Pandas.Series.str.strip — Pandas 2.0.0 Documentation, n.d.)ml",
      "metadata": {}
    },
    {
      "id": "529295b5-fafa-4429-86ed-14938dc64710",
      "cell_type": "code",
      "source": "# Remove 'county' and 'co' (case-insensitive) from all values in the 'County' column\nmerge_data['county'] = merge_data['county']\\\n    .str.replace(r'\\bcounty\\b|\\bco\\b|\\.', '', case=False, regex=True)\\\n    .str.strip()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "964f1e01-685c-4dbb-a3d4-e0004a8debbc",
      "cell_type": "code",
      "source": "merge_data.head(50)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "015a374a-9eff-43dd-85b5-00f2a6b6fd60",
      "cell_type": "code",
      "source": "# Remove '_' and '.' from 'Waste Category'\nmerge_data['waste_category'] = merge_data['waste_category'].str.replace('_', '').str.replace('.', '').str.strip().str.lower()\nmerge_data.head(25)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "16837d38-7e9f-4e49-b652-7dbc4befcf03",
      "cell_type": "code",
      "source": "merge_data['waste_category'] = merge_data['waste_category'].str.replace('hazardous', '',case=False).str.replace('non-', '',case=False) .str.replace('(e-Waste)', '',case=False).str.strip()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "05624989-7f4b-4455-96dd-12e2b7be2d79",
      "cell_type": "code",
      "source": "merge_data.head(20)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "778e92db-17f1-4388-8f7a-11dfa0c918ec",
      "cell_type": "code",
      "source": "# Unique values in each column individually\nunique_county = merge_data['county'].unique()\nunique_waste_type = merge_data['waste_type'].unique()\nunique_waste_category = merge_data['waste_category'].unique()\n\n# Print the results\nprint(\"Unique values in 'County':\", unique_county)\nprint(\"Unique values in 'Waste Type':\", unique_waste_type)\nprint(\"Unique values in 'Waste Category':\", unique_waste_category)\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "db67cc90-918d-439e-9782-1ac2c4b6f60a",
      "cell_type": "markdown",
      "source": "(Pandas.DataFrame.replace — Pandas 1.2.4 Documentation, n.d.)  \n(Working with Missing Data — Pandas 1.5.1 Documentation, n.d.)  \n(Pandas.DataFrame.map — Pandas 2.2.2 Documentation, n.d.)",
      "metadata": {}
    },
    {
      "id": "42b175fb-e46c-4d83-8f64-5e82fb8d2c96",
      "cell_type": "code",
      "source": "category_mapping = {\n    'waste industrial': 'Industrial Waste',\n    'i n d u s t r i a l  h a z a r d o u s  w a s t e': 'Industrial Waste',\n    'm e d i c a l  w a s t e': 'Medical Waste',\n    'medicalwaste': 'Medical Waste',\n    np.nan: np.nan,  # Keep NaN as is\n    'organic waste': 'Organic Waste',\n    'industrial  waste': 'Industrial Waste',\n    'chemical waste': 'Chemical Waste',\n    'municipal solid waste': 'Municipal Solid Waste',\n    'industrialwaste': 'Industrial Waste',\n    'electronicwaste': 'Electronic Waste',\n    'medical waste': 'Medical Waste',\n    'm u n i c i p a l  s o l i d  w a s t e': 'Municipal Solid Waste',\n    'industrial waste': 'Industrial Waste',\n    'construction and demolition waste': 'Construction And Demolition Waste',\n    'a g r i c u l t u r a l  w a s t e': 'Agricultural Waste',\n    'c h e m i c a l  w a s t e': 'Chemical Waste',\n    'i n d u s t r i a l  n o n -h a z a r d o u s  w a s t e': 'Industrial Waste',\n    'agricultural waste': 'Agricultural Waste',\n    'organicwaste': 'Organic Waste',\n    'chemicalwaste': 'Chemical Waste',\n    'and waste construction demolition': 'Construction And Demolition Waste',\n    'electronic waste': 'Electronic Waste',\n    'agriculturalwaste': 'Agricultural Waste',\n    'waste and construction demolition': 'Construction And Demolition Waste',\n    'c o n s t r u c t i o n  a n d  d e m o l i t i o n  w a s t e': 'Construction And Demolition Waste',\n    'municipal waste solid': 'Municipal Solid Waste',\n    'waste and demolition construction': 'Construction And Demolition Waste',\n    'constructionanddemolitionwaste': 'Construction And Demolition Waste',\n    'waste demolition and construction': 'Construction And Demolition Waste',\n    'waste organic': 'Organic Waste',\n    'electronic  waste': 'Electronic Waste',\n    'waste medical': 'Medical Waste',\n    'e l e c t r o n i c  w a s t e  (e -w a s t e )': 'Electronic Waste',\n    'waste agricultural': 'Agricultural Waste',\n    'waste electronic': 'Electronic Waste',\n    'waste municipal solid': 'Municipal Solid Waste',\n    'municipalsolidwaste': 'Municipal Solid Waste',\n    'waste chemical': 'Chemical Waste',\n    'waste construction demolition and': 'Construction And Demolition Waste',\n    'o r g a n i c  w a s t e': 'Organic Waste',\n    'demolition waste and construction': 'Construction And Demolition Waste',\n    'waste solid municipal': 'Municipal Solid Waste',\n    'construction waste and demolition': 'Construction And Demolition Waste',\n    'solid waste municipal': 'Municipal Solid Waste',\n    'and construction demolition waste': 'Construction And Demolition Waste',\n    'construction demolition waste and': 'Construction And Demolition Waste',\n    'and waste demolition construction': 'Construction And Demolition Waste',\n    'solid municipal waste': 'Municipal Solid Waste',\n    'construction waste demolition and': 'Construction And Demolition Waste',\n    'and demolition waste construction': 'Construction And Demolition Waste',\n    'and construction waste demolition': 'Construction And Demolition Waste',\n    'demolition construction waste and': 'Construction And Demolition Waste',\n    'demolition waste construction and': 'Construction And Demolition Waste',\n    'waste  electronic': 'Electronic Waste',\n    'construction and waste demolition': 'Construction And Demolition Waste',\n    'construction demolition and waste': 'Construction And Demolition Waste',\n    'waste demolition construction and': 'Construction And Demolition Waste',\n    'demolition and waste construction': 'Construction And Demolition Waste',\n    'demolition and construction waste': 'Construction And Demolition Waste',\n    'demolition construction and waste': 'Construction And Demolition Waste',\n    'waste construction and demolition': 'Construction And Demolition Waste',\n    'and demolition construction waste': 'Construction And Demolition Waste'\n}",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "69b4050c-41a9-427d-8514-6e6f7e615845",
      "cell_type": "code",
      "source": "merge_data['waste_category'] = merge_data['waste_category'].replace(category_mapping)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1bd42aa8-6b22-4b42-b056-06e0322d7c30",
      "cell_type": "code",
      "source": "county_mapping = {\n    'nan': np.nan,  # String 'nan'\n    np.nan: np.nan  # Explicit NaN\n}\n\nwaste_type_mapping = {\n    np.nan: np.nan  # Explicit NaN\n}\n\nwaste_category_mapping = {\n    'nan': np.nan,          # String 'nan'\n    'n a n': np.nan,        # String 'n a n'\n}\n\n# Apply mappings\nmerge_data['county'] = merge_data['county'].replace(county_mapping)\nmerge_data['waste_type'] = merge_data['waste_type'].replace(waste_type_mapping)\nmerge_data['waste_category'] = merge_data['waste_category'].replace(waste_category_mapping)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6ce18da3-df9d-4276-899b-393f8b2d4f88",
      "cell_type": "code",
      "source": "merge_data.head(50)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2815ee4c-f53b-414a-9521-a35862dcac8a",
      "cell_type": "code",
      "source": "# Unique values in each column individually\nunique_county = merge_data['county'].unique()\nunique_waste_type = merge_data['waste_type'].unique()\nunique_waste_category = merge_data['waste_category'].unique()\n\n# Print the results\nprint(\"Unique values in 'County':\", unique_county)\nprint(\"Unique values in 'Waste Type':\", unique_waste_type)\nprint(\"Unique values in 'Waste Category':\", unique_waste_category)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e56a4de6-5305-4cba-b661-e065be9e72da",
      "cell_type": "markdown",
      "source": "### C. Handling Numeric Data\n- Converted `waste_amount_in_tonnes` to numeric formats.  \n\n- Removed non-numeric characters (e.g., commas) since the  `waste_amount_in_tonnes` column is a numerical field.\n\n- All the invalid entries since they could cause errors or innacuracies during analysis were converted to NaN (Not a Number) so that we could easily locate and handle missing or invalid values in pandas.\n(Pandas.to_numeric — Pandas 1.4.2 Documentation, n.d.)  \n(Pandas.Series.str.replace — Pandas 2.0.3 Documentation, n.d.)\n",
      "metadata": {}
    },
    {
      "id": "fdfa2de5-5efe-457d-8081-25e21791f7e0",
      "cell_type": "code",
      "source": "merge_data['waste_amount_in_tonnes'] = pd.to_numeric(\n    merge_data['waste_amount_in_tonnes']\n    .str.replace(r'[^\\d.,]', '', regex=True) # Remove non-numeric characters\n    .str.replace(',', ''), # Remove commas\n    errors='coerce'  # Convert invalid entries to NaN\n)\nmerge_data",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b335c582-b82a-4a94-b5e8-88480d25b956",
      "cell_type": "markdown",
      "source": "- Removed extra spaces between words, replaced characters like \".\" , \"_\" with \" \" and adjusted inconsistent capitalization in `treatment_method`.  \n- Made the values in the `treatment_method`lower case for uniformity.\n(Pandas.Series.str.lower — Pandas 2.2.2 Documentation, n.d.) ",
      "metadata": {}
    },
    {
      "id": "7dd09859-d30d-45fa-9bf6-6f96e80f9a5c",
      "cell_type": "code",
      "source": "# Remove '_' and '.' from 'treatment_method'\nmerge_data['treatment_method'] = merge_data['treatment_method'].str.replace('_', '').str.replace('.', '').str.strip().str.lower()\nmerge_data",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3ecf055a-e177-4d0c-9f09-73e7e289ae8f",
      "cell_type": "markdown",
      "source": "- Finding the unique values in `treatement_method` column to detect any anomalies or unexpected values, inconsistent formatting (e.g.\"disposal - other\").\n- We then cleaned and standardized the values in the `treatement_method` column using the dictionary mapping to remove inconsistences (e.g. \"- landfill disposal\" to \"Disposal-Landfill\").",
      "metadata": {}
    },
    {
      "id": "8ea90c76-35a7-4982-81bb-86c127b12239",
      "cell_type": "code",
      "source": "merge_data['treatment_method'].unique()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4431e1d3-8af9-4ded-a6da-54085c782a0c",
      "cell_type": "code",
      "source": "treatment_mapping = {\n    'disposal - other': 'Disposal-Other',\n    'd i s p o s a l  - o t h e r': 'Disposal-Other',\n    'other - disposal': 'Disposal-Other',\n    '- other disposal': 'Disposal-Other',\n    'disposal other -': 'Disposal-Other',\n    '- disposal other': 'Disposal-Other',\n    'other disposal -': 'Disposal-Other',\n    'disposal-other': 'Disposal-Other',\n    'disposal - incineration': 'Disposal-Incineration',\n    'incineration disposal -': 'Disposal-Incineration',\n    'd i s p o s a l  - i n c i n e r a t i o n': 'Disposal-Incineration',\n    '- incineration disposal': 'Disposal-Incineration',\n    'disposal-incineration': 'Disposal-Incineration',\n    'incineration - disposal': 'Disposal-Incineration',\n    '- disposal incineration': 'Disposal-Incineration',\n    'disposal incineration -': 'Disposal-Incineration',\n    'disposal-landfill': 'Disposal-Landfill',\n    'landfill - disposal': 'Disposal-Landfill',\n    'disposal - landfill': 'Disposal-Landfill',\n    '- landfill disposal': 'Disposal-Landfill',\n    'landfill disposal -': 'Disposal-Landfill',\n    'd i s p o s a l  - l a n d f i l l': 'Disposal-Landfill',\n    '- disposal landfill': 'Disposal-Landfill',\n    'disposal landfill -': 'Disposal-Landfill',\n    'recovery - recycling': 'Recovery-Recycling',\n    'recovery-recycling': 'Recovery-Recycling',\n    '- recovery recycling': 'Recovery-Recycling',\n    'r e c o v e r y  - r e c y c l i n g': 'Recovery-Recycling',\n    'recycling - recovery': 'Recovery-Recycling',\n    '- recycling recovery': 'Recovery-Recycling',\n    'recovery recycling -': 'Recovery-Recycling',\n    'recycling recovery -': 'Recovery-Recycling',\n    'recovery - composting': 'Recovery-Composting',\n    '- composting recovery': 'Recovery-Composting',\n    'r e c o v e r y  - c o m p o s t i n g': 'Recovery-Composting',\n    'recovery-composting': 'Recovery-Composting',\n    'composting - recovery': 'Recovery-Composting',\n    'recovery composting -': 'Recovery-Composting',\n    '- recovery composting': 'Recovery-Composting',\n    'composting recovery -': 'Recovery-Composting',\n    'recovery - energy recovery': 'Recovery-Energy Recovery',\n    'recovery energy recovery -': 'Recovery-Energy Recovery',\n    'r e c o v e r y  - e n e r g y  r e c o v e r y': 'Recovery-Energy Recovery',\n    'recovery - recovery energy': 'Recovery-Energy Recovery',\n    'recovery recovery energy -': 'Recovery-Energy Recovery',\n    '- energy recovery recovery': 'Recovery-Energy Recovery',\n    '- recovery recovery energy': 'Recovery-Energy Recovery',\n    '- recovery energy recovery': 'Recovery-Energy Recovery',\n    'recovery recovery - energy': 'Recovery-Energy Recovery',\n    'energy recovery recovery -': 'Recovery-Energy Recovery',\n    'energy - recovery recovery': 'Recovery-Energy Recovery',\n    'energy recovery - recovery': 'Recovery-Energy Recovery',\n    'recovery-energyrecovery': 'Recovery-Energy Recovery',\n    'recovery energy - recovery': 'Recovery-Energy Recovery',\nnp.nan: np.nan,  # Keep NaN values as NaN\n    'nan': np.nan,\n    'n a n': np.nan}",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "9d98d7bc-4d6c-49e4-a3a1-a5540fd9aed3",
      "cell_type": "code",
      "source": "merge_data['treatment_method'] = merge_data['treatment_method'].replace(treatment_mapping)\nmerge_data.head(50)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "58164a6a-36f1-487a-871b-1ad2befe047a",
      "cell_type": "code",
      "source": "unique_entries=merge_data['treatment_method'].unique()\nfor entry in unique_entries:\n    print(entry)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "78eaa05f-99cb-4489-bd90-fa2e1f6fdf9f",
      "cell_type": "markdown",
      "source": "(Pandas.Series.value_counts — Pandas 1.3.4 Documentation, n.d.)",
      "metadata": {}
    },
    {
      "id": "e22c4679-d2b4-49c1-8799-54a55935e0a6",
      "cell_type": "code",
      "source": "value=merge_data['treatment_method'].value_counts()\nprint(value)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3198e277-d89a-44e9-bada-2af27e98eef9",
      "cell_type": "markdown",
      "source": "\n- Converted `price_of_treatment` to numeric formats using a custom function `convert_to_float` to clean the column.\n\n- Removed non-numeric characters (e.g., commas, € signs)\n  \n### Outcome\n- Uniform text values across categories for consistent grouping which will make analysis easier.\n- Accurate numeric values for analysis.\n- Removed redundancy in text format.\n\n(Pandas.DataFrame.round — Pandas 1.3.4 Documentation, n.d.)",
      "metadata": {}
    },
    {
      "id": "64e473ee-b9e3-41ce-8110-563d8368ee3f",
      "cell_type": "code",
      "source": "# Function to extract and convert price to float\ndef convert_to_float(value):\n    if isinstance(value, str):  # Process only if the value is a string\n        # Extract numeric value from the string\n        numeric_value = re.findall(r'[\\d\\.,]+', value)\n        \n        if numeric_value:\n            # Clean the numeric string (remove commas and extra spaces)\n            cleaned_value = numeric_value[0].replace(',', '').replace('€', '').strip()\n            \n            # Convert the cleaned string to float\n            return float(cleaned_value)\n        else:\n            return None  # If no valid numeric value is found\n    elif isinstance(value, (int, float)):  # If it's already numeric, return it as is\n        return value\n    else:\n        return None  # Handle any other unexpected types\n\n# Apply the function to the \"price_of_treatment\" column\nmerge_data['price_of_treatment'] = merge_data['price_of_treatment'].apply(convert_to_float)\n\nmerge_data['price_of_treatment'] = merge_data['price_of_treatment'].round(2)\n\n# Display the updated DataFrame\nprint(merge_data)\n\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "aafb210f-dc07-4f2b-b65f-ba18a9c56aa1",
      "cell_type": "markdown",
      "source": "# 3.Handling Missing Values",
      "metadata": {}
    },
    {
      "id": "dc607ffb-b07c-4a32-bdfe-9fc8e3c9b68f",
      "cell_type": "markdown",
      "source": "### A. Identifing columns with Missing Values:\n1. **treatment_method**: 21,950 missing values.\n2. **price_of _reatment**: 19,656 missing values.\n3. **waste_amount_in_tonnes**: 3,277 missing values.\n4. **Quarter**: 3,342 missing values.\n5. **county**: 3,269 missing values.\n6. **waste_type**: 3,312 missing values.\n7. **waste_category**: 3,242 missing values.",
      "metadata": {}
    },
    {
      "id": "a4a63e5f-94be-445d-b166-63bb7047f400",
      "cell_type": "markdown",
      "source": "(Pandas DataFrame Isnull() Method, n.d.)  \n(Pandas DataFrame Sum() Method, n.d.)",
      "metadata": {}
    },
    {
      "id": "205455ec-0351-420d-a331-7611e380f7ff",
      "cell_type": "code",
      "source": "print(merge_data.isnull().sum())",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4152307b-a8f8-4521-8de1-a72841fa21c8",
      "cell_type": "markdown",
      "source": "### B. Strategies Adopted to deal with the Missing Values:\n- **Forward Fill**: Filled missing value in categorical fields like `waste_category` and  `Quarter` using `ffill`.\n- **Forward Fill** and **Backward Fill**: Filled the missing values in  `treatment_method` with both methods because there were missing values present in the first and last row.\n  \n- **Imputation:**\n  - Median for numerical fields (e.g., `price_of_treatment`and `waste_amount_in_tonnes`) to prevent influence from extreme values.\n \n\n(Pandas DataFrame Fillna() Method, n.d.)  \n(Python Statistics.median() Method, n.d.)",
      "metadata": {}
    },
    {
      "id": "d978172e-3720-4e8c-8c1e-47c2d49e0c85",
      "cell_type": "code",
      "source": "merge_data['Quarter'] = merge_data['Quarter'].fillna(method='ffill')  # Forward fill\nmerge_data",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "269e0d23-fa11-4d3f-81d8-d890612bf2f3",
      "cell_type": "code",
      "source": "merge_data['county'] = merge_data['county'].fillna(method='ffill')\nmerge_data['waste_type'] = merge_data['waste_type'].fillna(method='ffill')\nmerge_data['waste_category'] = merge_data['waste_category'].fillna(method='ffill')\nmerge_data['treatment_method'] = merge_data['treatment_method'].fillna(method='ffill')\nmerge_data['treatment_method'] = merge_data['treatment_method'].fillna(method='bfill')\nmerge_data",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "40c44ff8-219b-4440-b2d9-d01a33f80f66",
      "cell_type": "code",
      "source": "merge_data['waste_amount_in_tonnes'] = merge_data['waste_amount_in_tonnes'].fillna(merge_data['waste_amount_in_tonnes'].median())\nmerge_data['price_of_treatment'] = merge_data['price_of_treatment'].fillna(merge_data['price_of_treatment'].median())\nmerge_data",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f0a56535-a662-44cf-bb39-971584348b4f",
      "cell_type": "code",
      "source": "print(merge_data.isnull().sum())",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3734092e-6736-4db2-b449-b407284d6df2",
      "cell_type": "markdown",
      "source": "### C. Removing Duplicates  \n- Removing the duplicate rows since they might affect analysis accuracy by inflating the importance of redundant data.\n- Also removing duplicate rows will ensure reliability of the dataset and make better visualizations.\n\n(Pandas DataFrame Duplicated() Method | Pandas Method, 2018) \n(Python | Pandas Dataframe.drop_duplicates(), 2018)\n",
      "metadata": {}
    },
    {
      "id": "fdb09900-030c-46a9-8b29-2df8cdac2b7a",
      "cell_type": "code",
      "source": "print(merge_data.duplicated().sum())",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "364ac672-6388-4e03-9647-962749edc047",
      "cell_type": "code",
      "source": "# Remove duplicate rows from the dataset\nmerge_data= merge_data.drop_duplicates()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "20f05231-5dca-4dd6-b7a1-6cf5cefad173",
      "cell_type": "code",
      "source": "print(merge_data.duplicated().sum())",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1d13e44a-8bfb-4461-94ba-306369b887e0",
      "cell_type": "code",
      "source": "merge_data['waste_amount_in_tonnes'].describe   #Calculating the mean before handling outliers to understand initial average.",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f394c090-da73-44e5-9f55-aa2ce7fde9b2",
      "cell_type": "markdown",
      "source": "# 4. Detecting Outliers in the Dataset\n Data points that substantially differ from the rest of the observations in a dataset are known as outliers. \n Finding outliers is essential to preserving the integrity of the data and creating reliable models.\n\n ---\n \n## A. Finding Outliers for Waste Amount in Tonnes\n\nThe following steps were used to identify outliers in the `waste_amount_in_tonnes` data.\n\n---\n\n### Steps to Identify Outliers\n\n1. **Calculate Quartiles (Q1 and Q3)**:  \n   The 25th percentile (Q1) and 75th percentile (Q3) were computed for the `waste_amount_in_tonnes` column.\n   \n2. **Calculate IQR**:  \n   The **Interquartile Range (IQR)** was determined as the difference between Q3 and Q1.\n   \n3. **Determine Outlier Bounds**:  \n   The outlier bounds were defined as:\n   - **Lower Bound**: \\( Q1 - 1.5 \\times IQR \\)\n   - **Upper Bound**: \\( Q3 + 1.5 \\times IQR \\)\n   \n4. **Identify Outliers**:  \n   The outliers were identified as values below the lower bound or above the upper bound.\n\n5. **Visualized Outliers**:  \n   The outliers in the `waste_amount_in_tonnes` data was identified and displayed by using boxplot and histogram.\n   \n\n---\n\n(Pandas.DataFrame.quantile — Pandas 2.1.1 Documentation, n.d.)  \n(Waskom, n.d.)  \n(Matplotlib.pyplot.hist — Matplotlib 3.5.1 Documentation, n.d.)  \n(GeeksforGeeks, 2021)  \n(Zach, 2020) \n",
      "metadata": {}
    },
    {
      "id": "7f6d71c1-de22-4bc6-bbee-3f6c964cdee4",
      "cell_type": "code",
      "source": "#Finding outliers for waste amount in tonnes\n# Calculate Q1 (25th percentile) and Q3 (75th percentile)\nQ1 = merge_data['waste_amount_in_tonnes'].quantile(0.25)\nQ3 = merge_data['waste_amount_in_tonnes'].quantile(0.75)\n\n# Calculate IQR\nIQR_1 = Q3 - Q1\n\n# Determine outlier bounds\nlower_bound_1 = Q1 - 1.5 * IQR_1\nupper_bound_1 = Q3 + 1.5 * IQR_1\n\n# Identify outliers\noutliers_1 = merge_data[(merge_data['waste_amount_in_tonnes'] <= lower_bound_1) | (merge_data['waste_amount_in_tonnes'] >= upper_bound_1)]\n\n# Print outliers\nprint(\"Identified Outliers:\")\nprint(outliers_1)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fff4d25f-2cc1-4bda-9107-1037d97fab56",
      "cell_type": "code",
      "source": "#boxplot\nsns.boxplot(data=merge_data, x='waste_amount_in_tonnes')\nplt.title('Boxplot for Identifying Outliers')\nplt.xlabel('outliers_1')\nplt.show()\n\n# Plot histogram\nplt.hist(merge_data['waste_amount_in_tonnes'], bins=10, color='skyblue', edgecolor='black')\nplt.title('Histogram Before Handling Outliers')\nplt.xlabel('outliers_1')\nplt.ylabel('Frequency')\nplt.show()\n ",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3fca0dc9-082d-4df4-b627-7f1193c3b3f7",
      "cell_type": "code",
      "source": "# Filter data to exclude outliers based on specified bounds\nmerge_data = merge_data[(merge_data['waste_amount_in_tonnes'] >= lower_bound_1) & (merge_data['waste_amount_in_tonnes'] <= upper_bound_1)]\nmerge_data",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "44155108-2ced-41b7-bc13-7f68bfd16c41",
      "cell_type": "markdown",
      "source": "(Pandas.DataFrame.describe — Pandas 0.20.2 Documentation, n.d.)  ",
      "metadata": {}
    },
    {
      "id": "d8083e35-f33b-4750-808c-0de2022f1009",
      "cell_type": "code",
      "source": "merge_data['waste_amount_in_tonnes'].describe()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2abdf804-a611-473e-a786-92ff8695a1fe",
      "cell_type": "markdown",
      "source": "## B. Visualizations  \n \n\n\n### Box Plot and Histogram for Waste Amount in Tonnes After Handling Outliers\n\nBelow are the visualizations showing the `waste_amount_in_tonnes` data after addressing outliers.\n\n---\n\n### Box Plot\n\nThe box plot displays the distribution of the waste amounts, indicating if any outliers remain after handling them. It visually shows the data's spread and identifies outliers as points outside the whiskers.\n\n---\n\n### Histogram\n\nThe histogram illustrates the frequency distribution of waste amounts, helping us understand how the data is distributed after handling outliers. It shows the number of occurrences within each bin and gives insights into the data's spread and central tendency.\n\n---\n\nThese visualizations confirm that outliers have been addressed and provide a clearer view of the data's distribution. \n\n(Waskom, n.d.)  \r(Matplotlib.pyplot.hist — Matplotlib 3.5.1 Documentation, n.d.)l",
      "metadata": {}
    },
    {
      "id": "677151ee-c096-4966-a711-73744e9df623",
      "cell_type": "code",
      "source": "# Plot the boxplot\nsns.boxplot(data=merge_data, x='waste_amount_in_tonnes')\nplt.title('Boxplot after fixing Outliers')\nplt.xlabel('outliers_1')\nplt.show()\n\n# Plot histogram\nplt.hist(merge_data['waste_amount_in_tonnes'], bins=10, color='skyblue', edgecolor='black')\nplt.title('Histogram after Handling Outliers')\nplt.xlabel('outliers_1')\nplt.ylabel('Frequency')\nplt.show()\nmerge_data",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3bbfd5da-afb2-41b5-bbd9-e091a5dd115c",
      "cell_type": "code",
      "source": "merge_data['price_of_treatment'].describe()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ae403db2-10ec-481e-9351-1f337787955c",
      "cell_type": "markdown",
      "source": "## Finding Outliers for Price of Treatment\n\nThe following steps were used to identify outliers in the `price_of_treatment` data.\n\n---\n\n## A. Steps to Identify Outliers\n\n1. **Calculate Quartiles (Q1 and Q3)**:  \n   The 25th percentile (Q1) and 75th percentile (Q3) were computed for the `price_of_treatment` column.\n   \n2. **Calculate IQR**:  \n   The **Interquartile Range (IQR)** was determined as the difference between Q3 and Q1.\n   \n3. **Determine Outlier Bounds**:  \n   The outlier bounds were defined as:\n   - **Lower Bound**: \\( Q1 - 1.5 \\times IQR \\)\n   - **Upper Bound**: \\( Q3 + 1.5 \\times IQR \\)\n   \n4. **Identify Outliers**:  \n   The outliers were identified as values below the lower bound or above the upper bound.\n\n5. **Print Identified Outliers**:  \n   The outliers in the `price_of_treatment` data were displayed.\n\n---",
      "metadata": {}
    },
    {
      "id": "eefc404b-9881-4e61-9458-5fdb0dd586b9",
      "cell_type": "code",
      "source": "#Finding outliers for price of treatment\n# Calculate Q1 (25th percentile) and Q3 (75th percentile)\nQt1 = merge_data['price_of_treatment'].quantile(0.25)\nQt3 = merge_data['price_of_treatment'].quantile(0.75)\n\n# Calculate IQR\nIQR_2 = Qt3 - Qt1\n\n# Determine outlier bounds\nlower_bound_2 = Qt1 - 1.5 * IQR_2\nupper_bound_2 = Qt3 + 1.5 * IQR_2\n\n# Identify outliers\noutliers_2 = merge_data[(merge_data['price_of_treatment'] <= lower_bound_2) | (merge_data['price_of_treatment'] >= upper_bound_2)]\n\n# Print outliers\nprint(\"Identified Outliers:\")\nprint(outliers_2)\nsns.boxplot(data=merge_data, x='price_of_treatment')\nplt.title('Boxplot for Identifying Outliers')\nplt.xlabel('outliers_2')\nplt.show()\n\n# Plot histogram\nplt.hist(merge_data['price_of_treatment'], bins=10, color='skyblue', edgecolor='black')\nplt.title('Histogram before Handling Outliers')\nplt.xlabel('outliers_2')\nplt.ylabel('Frequency')\nplt.show()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2635f839-8afe-4d71-8eda-fcdbf27e34dd",
      "cell_type": "markdown",
      "source": "(Pandas.DataFrame.clip — Pandas 2.1.4 Documentation, n.d.)",
      "metadata": {}
    },
    {
      "id": "47937db6-6238-49b3-9358-c1907cf095d7",
      "cell_type": "code",
      "source": "## Clip price values to be within the specified bounds\nmerge_data['price_of_treatment'] = merge_data['price_of_treatment'].clip(lower=lower_bound_2, upper=upper_bound_2)",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "84ede9f4-a281-4d58-a0db-c438d9b47f40",
      "cell_type": "markdown",
      "source": "## B. Visualizations  \n \n\n\n### Box Plot and Histogram for Price of Treatment After Handling Outliers\n\nBelow are the visualizations showing the `price_of_treatment` data after addressing outliers.\n\n---\n\n### Box Plot\n\nThe box plot below visualizes the `price_of_treatment` column and highlights any outliers:\n\n---\n\n### Histogram\n\nThe histogram shows the frequency distribution of `price_of_treatment` before handling outliers, helping in understanding how the data is distributed.\n\nThese visualizations help confirm the identification of outliers and provide insight into the data's distribution.",
      "metadata": {}
    },
    {
      "id": "5994e0ea-2f6e-4fb2-ab47-4ad8711b45d7",
      "cell_type": "code",
      "source": "# Plot the boxplot\nsns.boxplot(data=merge_data, x='price_of_treatment')\nplt.title('Boxplot after fixing Outliers')\nplt.xlabel('outliers_2')\nplt.show()\n\n# Plot histogram\nplt.hist(merge_data['price_of_treatment'], bins=10, color='skyblue', edgecolor='black')\nplt.title('Histogram after Handling Outliers')\nplt.xlabel('outliers_2')\nplt.ylabel('Frequency')\nplt.show()\nmerge_data",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c7318e15-def7-43f8-b2b3-d4f7002c6388",
      "cell_type": "code",
      "source": "merge_data['price_of_treatment'].describe()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4fd042de-2568-409e-a9aa-1e98687108d9",
      "cell_type": "markdown",
      "source": "The data before and after treating outliers is clearly understood with these visuals. The histogram displays the better distribution following outlier treatment, while the box plot illustrates the existence and magnitude of outliers in the `waste_amount_in_tonnes` and `price_of_treatment` data. Resolving outliers guarantees a more balanced dataset, lowering skewness and facilitating more precise and trustworthy analysis for making decisions.",
      "metadata": {}
    },
    {
      "id": "1af0ec73-8808-467f-bf2b-3fed3e05e86f",
      "cell_type": "markdown",
      "source": "## 5.Description of the calculated fields.  \n\nThe **Total Treatment Cost** and **Treatment Efficiency** computed fields were created to help organize and interpret the data. These measures provide a clearer picture of the data trends and comparisons, which streamlined the study. Various visualizations were used to find patterns, identify abnormalities, and comprehend linkages within the data during the **Exploratory Data Analysis (EDA)**. Bar charts made cross-category comparisons simple, box plots revealed outliers, and histograms offered information about the distribution of the data. The integrity of the data was preserved by using these visual aids and computations, and significant conclusions were drawn to help the project's objectives. perceptive analysis.  \n\n### Total Treatment Cost Calculation\nLarge amounts are made easier to handle and understand by calculating the total treatment cost in millions. `waste_amount_in_tonnes` is multiplied by `price_of_treatment` and then divided by 1,000,000 to convert the cost into millions, which facilitates the analysis and interpretation of the data.\n",
      "metadata": {}
    },
    {
      "id": "94d39ead-1e9f-4f83-9533-7f05ed8bd9ba",
      "cell_type": "code",
      "source": "#total_treatment_cost is in millions\nmerge_data['total_treat_cost_millions'] =( merge_data['waste_amount_in_tonnes'] * merge_data['price_of_treatment'])/1000000\nmerge_data['total_treat_cost_millions']=merge_data['total_treat_cost_millions'].round(2)\nmerge_data",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b7eb8505-1f30-494c-8251-f66182fc4182",
      "cell_type": "markdown",
      "source": "### Treatment Efficiency Calculation\nThe percentage of `waste_amount_in_tonnes` is rounded to two decimal places and compared to the dataset's maximum value is the treatment efficiency.\n(Pandas.DataFrame — Pandas 0.25.3 Documentation, 2020)",
      "metadata": {}
    },
    {
      "id": "da41688b-b8b0-4d84-a067-83cb338d3274",
      "cell_type": "code",
      "source": "merge_data['treatment_efficiency'] = (merge_data['waste_amount_in_tonnes'] / merge_data['waste_amount_in_tonnes'].max()) * 100\nmerge_data['treatment_efficiency']=merge_data['treatment_efficiency'].round(2)\nmerge_data",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "94c72df5-de5b-4fc0-a209-835b73a093b6",
      "cell_type": "markdown",
      "source": "## 6. Exploring the data visually using various graphs.\n\n### A. Quarterly Waste Amount Trends by Waste Type\nThe trends in waste amounts throughout various quarters, broken down by waste kind, are shown in this line plot. The data is displayed using **sns.lineplot**, with lines colored according to waste type for convenient comparison. A title, axis labels, and a legend placed outside for clarification are all part of the plot. The layout is modified to accommodate the plot elements, and the X-axis labels are rotated for improved readability.\n(Seaborn.lineplot — Seaborn 0.11.2 Documentation, n.d.)",
      "metadata": {}
    },
    {
      "id": "972004ce-cfa0-457f-9a5d-1a9c7b201e2f",
      "cell_type": "code",
      "source": "# Set figure size\nplt.figure(figsize=(14, 6))\n\n# Plot waste trends by quarter and type\nsns.lineplot(data=merge_data, x='Quarter', y='waste_amount_in_tonnes', hue='waste_type', errorbar=None)\n\n# Title and labels\nplt.title('Quarterly Waste Amount Trends by Waste Type', fontsize=16)\nplt.xlabel('Quarter', fontsize=12)\nplt.ylabel('Waste Amount (Tonnes)', fontsize=12)\n\n# Adjust legend and ticks\nplt.legend(title='Waste Type', bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.xticks(rotation=90)\n\n# Fit layout and show plot\nplt.tight_layout()\nplt.show()\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1d4c022a-fe11-42f7-a465-7e748e7946d9",
      "cell_type": "markdown",
      "source": "Hazardous waste consistently exceeds non-hazardous waste, fluctuating around 9,500–10,500 tonnes, while non-hazardous waste ranges from 8,000–8,500 tonnes. Both types show irregular patterns without a clear trend, likely due to seasonal or operational factors.",
      "metadata": {}
    },
    {
      "id": "e6d586fa-d8df-4b1e-9f17-b39bc88b9c1f",
      "cell_type": "markdown",
      "source": "### B. Waste Amount by Category and Type\n\nThis bar plot visualizes the waste amounts by category, differentiated by waste type. It uses **sns.barplot** to display the data, with the bars representing waste amounts for each category, and the legend indicating the waste types. The x-axis labels are rotated for readability, and the layout is adjusted for better spacing. \n(Seaborn.barplot — Seaborn 0.11.1 Documentation, n.d.) ",
      "metadata": {}
    },
    {
      "id": "9dc7279f-487f-4868-8e1d-eb209328b5d8",
      "cell_type": "code",
      "source": "# Set the figure size for the plot\nplt.figure(figsize=(14, 6))\n\n# Create a barplot showing waste amount by category and type\nsns.barplot(data=merge_data, x='waste_category', y='waste_amount_in_tonnes', hue='waste_type', errorbar=None)\n\n# Set the title for the plot\nplt.title('Waste Amount by Category and Type', fontsize=16)\n\n# Label the x-axis and y-axis\nplt.xlabel('Waste Category', fontsize=12)\nplt.ylabel('Waste Amount (Tonnes)', fontsize=12)\n\n# Adjust the legend position\nplt.legend(title='Waste Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n\n# Rotate x-axis labels for better readability\nplt.xticks(rotation=45)\n\n# Ensure layout fits into the figure area\nplt.tight_layout()\n\n# Show the plot\nplt.show()\n",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "51b30ea3-6e22-44bd-a4ad-801d4bb76253",
      "cell_type": "markdown",
      "source": "Hazardous waste dominates most categories, especially in chemical, industrial, and electronic waste, while non-hazardous waste closely matches in organic and construction waste. Overall, waste generation is balanced across both types with no drastic variations.\nhas context menu",
      "metadata": {}
    },
    {
      "id": "0596569d-ae06-4629-b619-10ecb830ebe7",
      "cell_type": "markdown",
      "source": "### C. Top 10 Counties by Total Waste Amount\n\nThe top ten counties with the highest overall trash amounts are displayed in this bar plot. The information is arranged in descending order and categorized by county. The plot has rotated x-axis labels for readability and a color scheme for aesthetic appeal. Better spacing is achieved by adjusting the layout. \n(Seaborn.barplot — Seaborn 0.11.1 Documentation, n.d.) ",
      "metadata": {}
    },
    {
      "id": "dd5581a1-8ef2-4ead-b36a-aa7d8d637d23",
      "cell_type": "code",
      "source": "# Get top 10 counties by waste amount\ntop_counties = merge_data.groupby('county')['waste_amount_in_tonnes'].sum().sort_values(ascending=False).head(10)\n\n# Plot bar chart of top counties\nplt.figure(figsize=(14, 6))\nsns.barplot(x= top_counties.index, y=top_counties.values, palette='plasma',hue= top_counties, legend= False)\n\n# Add title and labels\nplt.title('Top 10 Counties by Total Waste Amount', fontsize=16)\nplt.xlabel('County', fontsize=12)\nplt.ylabel('Total Waste Amount (Tonnes)', fontsize=12)\n\n# Rotate x-axis labels\nplt.xticks(rotation=45)\nplt.tight_layout()\n\n# Show plot\nplt.show()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7678ac56-ba6b-4cd3-957a-ddcb3bbf7c92",
      "cell_type": "markdown",
      "source": "The graph shows a relatively consistent waste generation among the top counties, with Wicklow slightly ahead. No sharp spikes or drastic drops are observed, indicating a similar waste production pattern across these counties.",
      "metadata": {}
    },
    {
      "id": "4b7966ba-c0dc-48bb-831d-ba3332e49d7d",
      "cell_type": "markdown",
      "source": "### D. Waste Distribution by Type\n\nThe distribution of total trash by kind is depicted in this pie chart, which also displays the percentage of each type. The y-axis label is buried for improved aesthetics, and the chart employs a pastel color scheme for visual clarity.\n\n(Pandas.DataFrame.plot.pie — Pandas 1.3.2 Documentation, n.d.)  \n(seaborn, 2013)",
      "metadata": {}
    },
    {
      "id": "b23b6a9f-1ebb-4270-8505-9181bb8b3a56",
      "cell_type": "code",
      "source": "# Group and sum waste amounts by type\nwaste_by_type = merge_data.groupby('waste_type')['waste_amount_in_tonnes'].sum()\n\n# Plot pie chart for waste distribution by type\nplt.figure(figsize=(8, 8))\nwaste_by_type.plot(kind='pie', autopct='%1.1f%%', startangle=140, colors=sns.color_palette('Pastel1'))\n\n# Title, remove y-label, and adjust layout\nplt.title('Pie Chart: Waste Distribution by Type', fontsize=16)\nplt.ylabel('')  # Hide y-label for better visual\nplt.tight_layout()\n\n# Show plot\nplt.show()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bc0fc481-cdca-4dac-a56c-bf7c511f9761",
      "cell_type": "markdown",
      "source": "The distribution of **Hazardous** and **Non-Hazardous** waste is depicted in the pie chart, which indicates that **42.7%** of the total waste is **Hazardous** and **57.3%** is **Non-Hazardous**. This suggests that most of the garbage handled in the dataset is less dangerous and may call for different management approaches because it shows a somewhat larger percentage of non-hazardous waste.",
      "metadata": {}
    },
    {
      "id": "2a5f4939-c4a3-4724-bae9-c591c0efa733",
      "cell_type": "markdown",
      "source": "### E. Distribution of Waste Amounts\nUsing a kernel density estimate (KDE) to visualize the general structure of the data, this histogram displays the distribution of waste quantities. It aids in comprehending how frequently various waste quantities occur within the dataset. \n\n(Seaborn.histplot — Seaborn 0.11.2 Documentation, n.d.)  ",
      "metadata": {}
    },
    {
      "id": "ad0966e5-7e81-40e8-8113-fe6813487c00",
      "cell_type": "code",
      "source": "# Create histogram with KDE for waste amount distribution\nplt.figure(figsize=(10, 6))\nsns.histplot(data=merge_data, x='waste_amount_in_tonnes', bins=30, kde=True, color='orange')\n\n# Set title and labels\nplt.title(' Distribution of Waste Amounts', fontsize=16)\nplt.xlabel('Waste Amount (Tonnes)', fontsize=12)\nplt.ylabel('Frequency', fontsize=12)\n\n# Adjust layout and show plot\nplt.tight_layout()\nplt.show()",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "60eba586-a860-46be-941c-29e7824b143b",
      "cell_type": "markdown",
      "source": "With most of the data concentrated between 0 and 10,000 tonnes, this histogram shows a positively skewed distribution of waste amounts, suggesting that smaller waste amounts are more common. The steady drop toward higher values raises the possibility of extreme outliers or infrequent instances of significant waste volumes, both of which merit more research",
      "metadata": {}
    },
    {
      "id": "8371e102-a6aa-45a3-ace6-994f6a95d9e3",
      "cell_type": "markdown",
      "source": "# 10. References\n\n1.\tNumpy. (2024). NumPy. Numpy.org. https://numpy.org/\r\n\r\n2.\tPandas. (2018). Python Data Analysis Library. Pydata.org. https://pandas.pydata.org/\r\n\r\n3.\tseaborn. (2012). seaborn: statistical data visualization — seaborn 0.9.0 documentation. Pydata.org. https://seaborn.pydata.org/\r\n\r\n4.\tMatplotlib. (2012). Matplotlib: Python plotting — Matplotlib 3.1.1 documentation. Matplotlib.org. https://matplotlib.org/\r\n\r\n5.\tPython Software Foundation. (2020). csv — CSV File Reading and Writing — Python 3.8.1 documentation. Python.org. https://docs.python.org/3/library/csv.html\r\n\r\n6.\tPython. (2009). re — Regular expression operations — Python 3.7.2 documentation. Python.org. https://docs.python.org/3/library/re.html\r\n\r\n7.\tpandas.merge — pandas 1.2.3 documentation. Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.merge.html\r\n\r\n8.\tpandas.DataFrame.rename — pandas 1.4.2 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html\r\n\r\n9.\tpandas.DataFrame.nunique — pandas 1.3.4 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.nunique.html\r\n\r\n10.\tpandas.to_datetime — pandas 1.3.4 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html\r\n\r\n11.\tpandas.NaT — pandas 2.2.3 documentation. (2024). Pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.NaT.html\r\n\r\n12.\tGeeksforGeeks. (2018, July 3). Python | Pandas.apply(). GeeksforGeeks; GeeksforGeeks. https://www.geeksforgeeks.org/python-pandas-apply/\r\n\r\n13.\tpandas.Series.str.replace — pandas 2.0.3 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.Series.str.replace.html\r\n\r\n14.\tpandas.Series.str.strip — pandas 2.0.0 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.Series.str.strip.html\r\n\r\n15.\tpandas.DataFrame.replace — pandas 1.2.4 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\r\n\r\n16.\tWorking with missing data — pandas 1.5.1 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/user_guide/missing_data.html\r\n\r\n17.\tpandas.DataFrame.map — pandas 2.2.2 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.map.html\r\n\r\n18.\tpandas.to_numeric — pandas 1.4.2 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html\r\n\r\n19.\tpandas.Series.str.replace — pandas 2.0.3 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.Series.str.replace.html\r\n\r\n20.\tpandas.Series.str.lower — pandas 2.2.2 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.Series.str.lower.html\r\n\r\n21.\tpandas.Series.value_counts — pandas 1.3.4 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html\r\n\r\n22.\tpandas.DataFrame.round — pandas 1.3.4 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.round.html\r\n\r\n23.\tPandas DataFrame isnull() Method. (n.d.). www.w3schools.com. https://www.w3schools.com/python/pandas/ref_df_isnull.asp\r\n\r\n24.\tPandas DataFrame sum() Method. (n.d.). www.w3schools.com. https://www.w3schools.com/python/pandas/ref_df_sum.asp\r\n\r\n25.\tPandas DataFrame fillna() Method. (n.d.). Www.w3schools.com. https://www.w3schools.com/python/pandas/ref_df_fillna.asp\r\n\r\n26.\tPython statistics.median() Method. (n.d.). Www.w3schools.com. https://www.w3schools.com/python/ref_stat_median.asp\r\n\r\n27.\tPandas DataFrame duplicated() Method | Pandas Method. (2018, July 23). GeeksforGeeks. https://www.geeksforgeeks.org/pandas-dataframe-duplicated/\r\n\r\n28.\tPython | Pandas dataframe.drop_duplicates(). (2018, August 2). GeeksforGeeks. https://www.geeksforgeeks.org/python-pandas-dataframe-drop_duplicates/\r\n\r\n29.\tpandas.DataFrame.quantile — pandas 2.1.1 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.quantile.html\r\n\r\n30.\tWaskom, M. (n.d.). seaborn.boxplot — seaborn 0.11.1 documentation. Seaborn.pydata.org. https://seaborn.pydata.org/generated/seaborn.boxplot.html\r\n\r\n31.\tmatplotlib.pyplot.hist — Matplotlib 3.5.1 documentation. (n.d.). Matplotlib.org. https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\r\n\r\n32.\tGeeksforGeeks. (2021, February 23). Detect and Remove the Outliers using Python. GeeksforGeeks. https://www.geeksforgeeks.org/detect-and-remove-the-outliers-using-python/\r\n\r\n33.\tZach. (2020, July 6). How to Remove Outliers in Python. Statology. https://www.statology.org/remove-outliers-python/\r\n\r\n34.\tpandas.DataFrame.describe — pandas 0.20.2 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/pandas-docs/version/0.20.2/generated/pandas.DataFrame.describe.html\r\n\r\n35.\tWaskom, M. (n.d.). seaborn.boxplot — seaborn 0.11.1 documentation. Seaborn.pydata.org. https://seaborn.pydata.org/generated/seaborn.boxplot.html\r\n\r\n36.\tmatplotlib.pyplot.hist — Matplotlib 3.5.1 documentation. (n.d.). Matplotlib.org. https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\r\n\r\n37.\tpandas.DataFrame.clip — pandas 2.1.4 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.clip.html\r\n\r\n38.\tpandas.DataFrame — pandas 0.25.3 documentation. (2020). Pydata.org. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\r\n\r\n39.\tseaborn.lineplot — seaborn 0.11.2 documentation. (n.d.). Seaborn.pydata.org. https://seaborn.pydata.org/generated/seaborn.lineplot.html\r\n\r\n40.\tseaborn.barplot — seaborn 0.11.1 documentation. (n.d.). Seaborn.pydata.org. https://seaborn.pydata.org/generated/seaborn.barplot.html\r\n\r\n41.\tpandas.DataFrame.plot.pie — pandas 1.3.2 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.pie.html\r\n\r\n42.\tseaborn. (2013). Choosing color palettes — seaborn 0.9.0 documentation. Pydata.org. https://seaborn.pydata.org/tutorial/color_palettes.html\r\n\r\n43.\tseaborn.histplot — seaborn 0.11.2 documentation. (n.d.). Seaborn.pydata.org. https://seaborn.pydata.org/generated/seaborn.histplot.html\r\n\r\n44.\t‌ Grammarly. (2). Grammarly. Grammarly.com. [AI-based proofreading tool] https://www.grammarly.com\r\n\r\norn.pydata.org. https://seaborn.pydata.org/generated/seaborn.histplot.html\r\n\r\n44.\t‌\r\n\r\n\r\n\r\n‌",
      "metadata": {}
    },
    {
      "id": "1be81c46-7063-45dd-b662-16e0f23a385f",
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "outputs": [],
      "execution_count": null
    }
  ]
}
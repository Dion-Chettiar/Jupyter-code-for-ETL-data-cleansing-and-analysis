{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3b413cb-df84-4b2d-8428-b76430b3b3cd",
   "metadata": {},
   "source": [
    "# IS6061_PYTHON_ASSIGNMENT_GROUP_PROJECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28dcac3-6841-48c4-b6e7-180926b1d6db",
   "metadata": {},
   "source": [
    "## GROUP 26  \n",
    "### GROUP MEMBERS  \n",
    "<div style=\"text-align: left;\">\n",
    "    \n",
    "| Name                   | ID         | \n",
    "|:-----------------------|:-----------|\n",
    "| **Praveen Mangawa**  | 124114478  | \n",
    "| **Mrunal Howale**      | 124112093  | \n",
    "| **Aida George**        | 124109805  | \n",
    "| **Shakirat Ezinne Muibi** | 123102416  | \n",
    "| **Dion Vincent Chettiar** | 124112586  | \n",
    "| **Dyuti Paresh Kamdar**   | 124101898  | \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214cca07-ceb2-4ad0-9d94-5ba5e78f1164",
   "metadata": {},
   "source": [
    "## Project Approach and Workflow\n",
    "\n",
    "In this group project, **the quarterly_waste_generation.csv** and **quarterly_waste_treatment.csv** datasets will be thoroughly analyzed. The objective is to merge these datasets using common fields like **Quarter**, **County**, **Waste Type**, **Waste Category**, and **Waste Amount (tonnes)** in order to create a single, cohesive dataset. **Exploratory Data Analysis (EDA)**, data aggregation and cleansing, data import, and insight generation are all steps in the data analytics lifecycle, which this project will adhere to.\r\n",
    "\r\n",
    "We will investigate different Python modules to process the data, deal with outliers and missing values, generate calculated fields, and display the data throughout the project. The EDA phase will fix any problems that arose during the cleaning and merging process and assist in identifying trends or patterns in the data. This report will give a thorough explanation of each phase, including the justification for the methodologies used, the difficulties encountered, and the analysis's ultimate conclusions.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1134c8-7cef-468b-a6e1-1ec489df9362",
   "metadata": {},
   "source": [
    "### Importing Necessary Libraries\r",
    "   \n",
    "The required libraries for managing, manipulating, and visualizing data are imported in this part\n",
    "\n",
    "   NumPy: (Numpy, 2024)    \n",
    "   Pandas: (Pandas, 2018)  \n",
    "   Seaborn: (seaborn, 2012)  \n",
    "   Matplotlib: (Matplotlib, 2012)  \n",
    "   CSV (Python module): (Python Software Foundation, 2020)  \n",
    "   Regular Expressions (re module): (Python, 2009)\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f88f853c-8f16-4aae-a7ef-de808103a011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd493f8f-cf50-4df4-b372-7273409722a1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'quarterly_waste_generation.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Read the CSV file containing quarterly waste generation data into a Pandas DataFrame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m wgen\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquarterly_waste_generation.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'quarterly_waste_generation.csv'"
     ]
    }
   ],
   "source": [
    "# Read the CSV file containing quarterly waste generation data into a Pandas DataFrame\n",
    "wgen=pd.read_csv(\"quarterly_waste_generation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf16f0-9af7-4b85-a455-048976c4c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file containing quarterly waste treatment data into a Pandas DataFrame\n",
    "wtreat=pd.read_csv(\"quarterly_waste_treatment.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd0777f-9ca1-48aa-ab20-7b6128ea3c0f",
   "metadata": {},
   "source": [
    "# 1. Dataset Description and Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6e67d-6f02-4edb-9d0b-f41e8b93843f",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "Decided to merges two datasets: **quarterly_waste_generation.csv** and **quarterly_waste_treatment.csv**, using the columns `Quarter`, `County`, `Waste Type`, `Waste Category`, and `Waste Amount (tonnes)` to create a unified dataset.\n",
    "(Pandas.merge — Pandas 1.2.3 Documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84816b3-e088-4576-832e-7894feaf54c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data = pd.merge(wgen, wtreat, on=['Quarter', 'County','Waste Type', 'Waste Category','Waste Amount (tonnes)'], how='outer')\n",
    "merge_data.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0135f29-d68b-4939-9a87-52878fe4e94e",
   "metadata": {},
   "source": [
    "Giving different names to the the columns in the merged datasets for consistency and clarity (e.g., `Waste Amount (tonnes)` to `waste_amount_in_tonnes`)\n",
    "\n",
    "### New Column Names\n",
    "1. **Quarter:** Time period for waste generation/treatment.\n",
    "2. **county:** Geographical region in ireland where waste was recorded.\n",
    "3. **waste_type:** Classification as Hazardous or Non-Hazardous.\n",
    "4. **waste_category:** Specific type of waste (e.g., Chemical Waste, Agricultural Waste).\n",
    "5. **waste_amount_in_tonnes:** Quantity of waste generated or treated.\n",
    "6. **treatment_method:** Process used for treating waste.\n",
    "7. **price_of_treatment:** Cost per tonne for treatment.\n",
    "\n",
    "(Pandas.DataFrame.rename — Pandas 1.4.2 Documentation, n.d.)  \n",
    "(Pandas.DataFrame.nunique — Pandas 1.3.4 Documentation, n.d.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1dca5a-8549-4260-bc62-da2debf73c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data.rename(columns={'Waste Amount (tonnes)': 'waste_amount_in_tonnes', 'Quarter': 'Quarter', 'County':'county', 'Waste Type' : 'waste_type', 'Waste Category': 'waste_category', 'Treatment Method':'treatment_method','Price of Treatment (€ per tonne)':'price_of_treatment'},inplace=True)\n",
    "merge_data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab4258-c798-411e-a459-2791e526e41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806f0cfa-c763-4974-9c62-c6dc3aa067ff",
   "metadata": {},
   "source": [
    "### Observed Issues\n",
    "1. **Date Inconsistencies:** The `Quarter` column contains inconsistent date formats.\n",
    "2. **Irregular Text Formats:** Variations in capitalization and special characters were present in  `treatment_method` , `county` and `waste_category`.\n",
    "3. **Missing Data:** Null values were present in all the columns.\n",
    "4. **Potential Duplicates:** There were duplicate rows in the dataset.\n",
    "5. **Outliers:** Possible extreme values in `waste_amount_in_tonnes` and`price_of_treatment`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d8497c-2b90-4af6-afa7-cf6d1b42ad17",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b03c78-38f9-4c1a-8d23-5e20eb6dca5d",
   "metadata": {},
   "source": [
    "### A. Standardizing Date Formats\n",
    "- A custom function `normalize_date` was implemented to unify date formats in the `Quarter` column.\n",
    "  \n",
    "- Formats handled include:\n",
    "  - `\"YYYY Qn\"` (e.g., \"2004 Q1\")\n",
    "  - `\"YYYYQn\"` (e.g., \"2004Q1\")\n",
    "  - `\"dd-MM-YYYY\"`(e.g \"01-01-2004\")\n",
    "\n",
    "(Pandas.to_datetime — Pandas 1.3.4 Documentation, n.d.)  \n",
    "(Pandas.NaT — Pandas 2.2.3 Documentation, 2024)  \n",
    "(GeeksforGeeks, 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d055e4-3359-4357-8ee3-d7caffbd05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize date formats\n",
    "def normalize_date(date_str):\n",
    "    if isinstance(date_str, str):\n",
    "        date_str = date_str.strip()  # Remove any leading/trailing whitespace\n",
    "\n",
    "        # Explicitly check for 'dd-Mon-yy' format (e.g., '01-Jan-04')\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format='%d-%b-%y', dayfirst=True)\n",
    "        except ValueError:\n",
    "            pass  # If it fails, continue to other formats\n",
    "\n",
    "        # Check for 'YYYYQn' format (e.g., '2004Q1')\n",
    "        match = re.match(r'(\\d{4})Q([1-4])', date_str)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            quarter = int(match.group(2))\n",
    "            # Map to start of the quarter\n",
    "            if quarter == 1:\n",
    "                return pd.to_datetime(f'01/01/{year}', dayfirst=True)\n",
    "            elif quarter == 2:\n",
    "                return pd.to_datetime(f'01/04/{year}', dayfirst=True)\n",
    "            elif quarter == 3:\n",
    "                return pd.to_datetime(f'01/07/{year}', dayfirst=True)\n",
    "            elif quarter == 4:\n",
    "                return pd.to_datetime(f'01/10/{year}', dayfirst=True)\n",
    "\n",
    "        # Check for 'YYYY Qn' or 'Qn YYYY' formats (e.g., '2004 Q1' or 'Q1 2004')\n",
    "        match = re.match(r'(\\d{4})\\s*Q([1-4])|Q([1-4])\\s*(\\d{4})', date_str)\n",
    "        if match:\n",
    "            year = int(match.group(1) or match.group(4))\n",
    "            quarter = int(match.group(2) or match.group(3))\n",
    "            # Map to start of the quarter\n",
    "            if quarter == 1:\n",
    "                return pd.to_datetime(f'01/01/{year}', dayfirst=True)\n",
    "            elif quarter == 2:\n",
    "                return pd.to_datetime(f'01/04/{year}', dayfirst=True)\n",
    "            elif quarter == 3:\n",
    "                return pd.to_datetime(f'01/07/{year}', dayfirst=True)\n",
    "            elif quarter == 4:\n",
    "                return pd.to_datetime(f'01/10/{year}', dayfirst=True)\n",
    "\n",
    "        # Check for 'Month dd, yyyy' format (e.g., 'January 01, 2004')\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format='%B %d, %Y', dayfirst=True)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        # Check for 'Month dd yyyy' format (e.g., 'January 01 2004')\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format='%B %d %Y', dayfirst=True)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        # Check for 'dd-Mon-yyyy' format (e.g., '01-Jan-2004')\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format='%d-%b-%Y', dayfirst=True)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        # Check for 'dd-mm-yyyy' format (e.g., '01-01-2004')\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format='%d-%m-%Y', dayfirst=True)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        # Check for 'dd/mm/yyyy' format\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format='%d/%m/%Y', dayfirst=True)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        # Check for 'mm/dd/yyyy' format\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format='%m/%d/%Y', dayfirst=False)\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        # Fallback for any remaining date strings\n",
    "        try:\n",
    "            return pd.to_datetime(date_str)\n",
    "        except ValueError:\n",
    "            return pd.NaT  # Return NaT if all parsing attempts fail\n",
    "    else:\n",
    "        return pd.NaT  # Return NaT for non-string entries\n",
    "\n",
    "# Normalize the 'Quarter' column to a single date format\n",
    "merge_data['Quarter'] = merge_data['Quarter'].apply(normalize_date)\n",
    "\n",
    "# Ensure the 'Quarter' column contains only datetime objects\n",
    "merge_data['Quarter'] = pd.to_datetime(merge_data['Quarter'], errors='coerce')\n",
    "\n",
    "# Replace specific dates with corresponding quarters\n",
    "def replace_with_quarters(date):\n",
    "    if pd.isnull(date):  # Handle NaT values\n",
    "        return date\n",
    "    elif date.day == 1 and date.month == 1:\n",
    "        return f\"Q1 {date.year}\"\n",
    "    elif date.day == 1 and date.month == 4:\n",
    "        return f\"Q2 {date.year}\"\n",
    "    elif date.day == 1 and date.month == 7:\n",
    "        return f\"Q3 {date.year}\"\n",
    "    elif date.day == 1 and date.month == 10:\n",
    "        return f\"Q4 {date.year}\"\n",
    "    else:\n",
    "        return date.strftime('%d/%m/%Y')  # Retain other dates in 'dd/mm/yyyy' format\n",
    "\n",
    "# Apply the quarter replacement function\n",
    "merge_data['Quarter'] = merge_data['Quarter'].apply(replace_with_quarters)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"\\nNormalized Data:\")\n",
    "print(merge_data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b80d12e-818f-4fca-8d9c-1102c8983b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4a3c46-da3b-41d3-b65a-4f8995c272c4",
   "metadata": {},
   "source": [
    "### B. Standardizing Text Fields\n",
    "- Removed redundant prefixes like \"county\" and \"co.\" from the `county` column using regex.\n",
    "  \n",
    "- Capitalized the values in `county`\n",
    "  \n",
    "- Finding the unique values in `waste_category`, `waste_type` and `county`. The purpose of the \"unique() method\" is to know the range of values especially in categorical columns, identifying missing values if NaN appears in the output and to check if the values in the column don't have any issues (e.g. the wording of the values, unwanted characters like \".\" inbetween each word)\n",
    "- Removed extra spaces between words, replaced characters like \".\" and  \"_\"  with  \" \", then adjusted inconsistent capitalization in `waste_category`.\n",
    "- Using the mapping method we corrected incomplete and inconsistent entries in the column `waste_category` like \"Waste Agricultural\" to \"Agricultural Waste\".\n",
    "\n",
    "(Pandas.Series.str.replace — Pandas 2.0.3 Documentation, n.d.) (Pandas.Series.str.strip — Pandas 2.0.0 Documentation, n.d.)ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529295b5-fafa-4429-86ed-14938dc64710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'county' and 'co' (case-insensitive) from all values in the 'County' column\n",
    "merge_data['county'] = merge_data['county']\\\n",
    "    .str.replace(r'\\bcounty\\b|\\bco\\b|\\.', '', case=False, regex=True)\\\n",
    "    .str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f1e01-685c-4dbb-a3d4-e0004a8debbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a374a-9eff-43dd-85b5-00f2a6b6fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '_' and '.' from 'Waste Category'\n",
    "merge_data['waste_category'] = merge_data['waste_category'].str.replace('_', '').str.replace('.', '').str.strip().str.lower()\n",
    "merge_data.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16837d38-7e9f-4e49-b652-7dbc4befcf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data['waste_category'] = merge_data['waste_category'].str.replace('hazardous', '',case=False).str.replace('non-', '',case=False) .str.replace('(e-Waste)', '',case=False).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05624989-7f4b-4455-96dd-12e2b7be2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e92db-17f1-4388-8f7a-11dfa0c918ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values in each column individually\n",
    "unique_county = merge_data['county'].unique()\n",
    "unique_waste_type = merge_data['waste_type'].unique()\n",
    "unique_waste_category = merge_data['waste_category'].unique()\n",
    "\n",
    "# Print the results\n",
    "print(\"Unique values in 'County':\", unique_county)\n",
    "print(\"Unique values in 'Waste Type':\", unique_waste_type)\n",
    "print(\"Unique values in 'Waste Category':\", unique_waste_category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db67cc90-918d-439e-9782-1ac2c4b6f60a",
   "metadata": {},
   "source": [
    "(Pandas.DataFrame.replace — Pandas 1.2.4 Documentation, n.d.)  \n",
    "(Working with Missing Data — Pandas 1.5.1 Documentation, n.d.)  \n",
    "(Pandas.DataFrame.map — Pandas 2.2.2 Documentation, n.d.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b175fb-e46c-4d83-8f64-5e82fb8d2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = {\n",
    "    'waste industrial': 'Industrial Waste',\n",
    "    'i n d u s t r i a l  h a z a r d o u s  w a s t e': 'Industrial Waste',\n",
    "    'm e d i c a l  w a s t e': 'Medical Waste',\n",
    "    'medicalwaste': 'Medical Waste',\n",
    "    np.nan: np.nan,  # Keep NaN as is\n",
    "    'organic waste': 'Organic Waste',\n",
    "    'industrial  waste': 'Industrial Waste',\n",
    "    'chemical waste': 'Chemical Waste',\n",
    "    'municipal solid waste': 'Municipal Solid Waste',\n",
    "    'industrialwaste': 'Industrial Waste',\n",
    "    'electronicwaste': 'Electronic Waste',\n",
    "    'medical waste': 'Medical Waste',\n",
    "    'm u n i c i p a l  s o l i d  w a s t e': 'Municipal Solid Waste',\n",
    "    'industrial waste': 'Industrial Waste',\n",
    "    'construction and demolition waste': 'Construction And Demolition Waste',\n",
    "    'a g r i c u l t u r a l  w a s t e': 'Agricultural Waste',\n",
    "    'c h e m i c a l  w a s t e': 'Chemical Waste',\n",
    "    'i n d u s t r i a l  n o n -h a z a r d o u s  w a s t e': 'Industrial Waste',\n",
    "    'agricultural waste': 'Agricultural Waste',\n",
    "    'organicwaste': 'Organic Waste',\n",
    "    'chemicalwaste': 'Chemical Waste',\n",
    "    'and waste construction demolition': 'Construction And Demolition Waste',\n",
    "    'electronic waste': 'Electronic Waste',\n",
    "    'agriculturalwaste': 'Agricultural Waste',\n",
    "    'waste and construction demolition': 'Construction And Demolition Waste',\n",
    "    'c o n s t r u c t i o n  a n d  d e m o l i t i o n  w a s t e': 'Construction And Demolition Waste',\n",
    "    'municipal waste solid': 'Municipal Solid Waste',\n",
    "    'waste and demolition construction': 'Construction And Demolition Waste',\n",
    "    'constructionanddemolitionwaste': 'Construction And Demolition Waste',\n",
    "    'waste demolition and construction': 'Construction And Demolition Waste',\n",
    "    'waste organic': 'Organic Waste',\n",
    "    'electronic  waste': 'Electronic Waste',\n",
    "    'waste medical': 'Medical Waste',\n",
    "    'e l e c t r o n i c  w a s t e  (e -w a s t e )': 'Electronic Waste',\n",
    "    'waste agricultural': 'Agricultural Waste',\n",
    "    'waste electronic': 'Electronic Waste',\n",
    "    'waste municipal solid': 'Municipal Solid Waste',\n",
    "    'municipalsolidwaste': 'Municipal Solid Waste',\n",
    "    'waste chemical': 'Chemical Waste',\n",
    "    'waste construction demolition and': 'Construction And Demolition Waste',\n",
    "    'o r g a n i c  w a s t e': 'Organic Waste',\n",
    "    'demolition waste and construction': 'Construction And Demolition Waste',\n",
    "    'waste solid municipal': 'Municipal Solid Waste',\n",
    "    'construction waste and demolition': 'Construction And Demolition Waste',\n",
    "    'solid waste municipal': 'Municipal Solid Waste',\n",
    "    'and construction demolition waste': 'Construction And Demolition Waste',\n",
    "    'construction demolition waste and': 'Construction And Demolition Waste',\n",
    "    'and waste demolition construction': 'Construction And Demolition Waste',\n",
    "    'solid municipal waste': 'Municipal Solid Waste',\n",
    "    'construction waste demolition and': 'Construction And Demolition Waste',\n",
    "    'and demolition waste construction': 'Construction And Demolition Waste',\n",
    "    'and construction waste demolition': 'Construction And Demolition Waste',\n",
    "    'demolition construction waste and': 'Construction And Demolition Waste',\n",
    "    'demolition waste construction and': 'Construction And Demolition Waste',\n",
    "    'waste  electronic': 'Electronic Waste',\n",
    "    'construction and waste demolition': 'Construction And Demolition Waste',\n",
    "    'construction demolition and waste': 'Construction And Demolition Waste',\n",
    "    'waste demolition construction and': 'Construction And Demolition Waste',\n",
    "    'demolition and waste construction': 'Construction And Demolition Waste',\n",
    "    'demolition and construction waste': 'Construction And Demolition Waste',\n",
    "    'demolition construction and waste': 'Construction And Demolition Waste',\n",
    "    'waste construction and demolition': 'Construction And Demolition Waste',\n",
    "    'and demolition construction waste': 'Construction And Demolition Waste'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b4050c-41a9-427d-8514-6e6f7e615845",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data['waste_category'] = merge_data['waste_category'].replace(category_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd42aa8-6b22-4b42-b056-06e0322d7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "county_mapping = {\n",
    "    'nan': np.nan,  # String 'nan'\n",
    "    np.nan: np.nan  # Explicit NaN\n",
    "}\n",
    "\n",
    "waste_type_mapping = {\n",
    "    np.nan: np.nan  # Explicit NaN\n",
    "}\n",
    "\n",
    "waste_category_mapping = {\n",
    "    'nan': np.nan,          # String 'nan'\n",
    "    'n a n': np.nan,        # String 'n a n'\n",
    "}\n",
    "\n",
    "# Apply mappings\n",
    "merge_data['county'] = merge_data['county'].replace(county_mapping)\n",
    "merge_data['waste_type'] = merge_data['waste_type'].replace(waste_type_mapping)\n",
    "merge_data['waste_category'] = merge_data['waste_category'].replace(waste_category_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce18da3-df9d-4276-899b-393f8b2d4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2815ee4c-f53b-414a-9521-a35862dcac8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values in each column individually\n",
    "unique_county = merge_data['county'].unique()\n",
    "unique_waste_type = merge_data['waste_type'].unique()\n",
    "unique_waste_category = merge_data['waste_category'].unique()\n",
    "\n",
    "# Print the results\n",
    "print(\"Unique values in 'County':\", unique_county)\n",
    "print(\"Unique values in 'Waste Type':\", unique_waste_type)\n",
    "print(\"Unique values in 'Waste Category':\", unique_waste_category)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56a4de6-5305-4cba-b661-e065be9e72da",
   "metadata": {},
   "source": [
    "### C. Handling Numeric Data\n",
    "- Converted `waste_amount_in_tonnes` to numeric formats.  \n",
    "\n",
    "- Removed non-numeric characters (e.g., commas) since the  `waste_amount_in_tonnes` column is a numerical field.\n",
    "\n",
    "- All the invalid entries since they could cause errors or innacuracies during analysis were converted to NaN (Not a Number) so that we could easily locate and handle missing or invalid values in pandas.\n",
    "(Pandas.to_numeric — Pandas 1.4.2 Documentation, n.d.)  \n",
    "(Pandas.Series.str.replace — Pandas 2.0.3 Documentation, n.d.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa2de5-5efe-457d-8081-25e21791f7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data['waste_amount_in_tonnes'] = pd.to_numeric(\n",
    "    merge_data['waste_amount_in_tonnes']\n",
    "    .str.replace(r'[^\\d.,]', '', regex=True) # Remove non-numeric characters\n",
    "    .str.replace(',', ''), # Remove commas\n",
    "    errors='coerce'  # Convert invalid entries to NaN\n",
    ")\n",
    "merge_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335c582-b82a-4a94-b5e8-88480d25b956",
   "metadata": {},
   "source": [
    "- Removed extra spaces between words, replaced characters like \".\" , \"_\" with \" \" and adjusted inconsistent capitalization in `treatment_method`.  \n",
    "- Made the values in the `treatment_method`lower case for uniformity.\n",
    "(Pandas.Series.str.lower — Pandas 2.2.2 Documentation, n.d.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd09859-d30d-45fa-9bf6-6f96e80f9a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove '_' and '.' from 'treatment_method'\n",
    "merge_data['treatment_method'] = merge_data['treatment_method'].str.replace('_', '').str.replace('.', '').str.strip().str.lower()\n",
    "merge_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecf055a-e177-4d0c-9f09-73e7e289ae8f",
   "metadata": {},
   "source": [
    "- Finding the unique values in `treatement_method` column to detect any anomalies or unexpected values, inconsistent formatting (e.g.\"disposal - other\").\n",
    "- We then cleaned and standardized the values in the `treatement_method` column using the dictionary mapping to remove inconsistences (e.g. \"- landfill disposal\" to \"Disposal-Landfill\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea90c76-35a7-4982-81bb-86c127b12239",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data['treatment_method'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4431e1d3-8af9-4ded-a6da-54085c782a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment_mapping = {\n",
    "    'disposal - other': 'Disposal-Other',\n",
    "    'd i s p o s a l  - o t h e r': 'Disposal-Other',\n",
    "    'other - disposal': 'Disposal-Other',\n",
    "    '- other disposal': 'Disposal-Other',\n",
    "    'disposal other -': 'Disposal-Other',\n",
    "    '- disposal other': 'Disposal-Other',\n",
    "    'other disposal -': 'Disposal-Other',\n",
    "    'disposal-other': 'Disposal-Other',\n",
    "    'disposal - incineration': 'Disposal-Incineration',\n",
    "    'incineration disposal -': 'Disposal-Incineration',\n",
    "    'd i s p o s a l  - i n c i n e r a t i o n': 'Disposal-Incineration',\n",
    "    '- incineration disposal': 'Disposal-Incineration',\n",
    "    'disposal-incineration': 'Disposal-Incineration',\n",
    "    'incineration - disposal': 'Disposal-Incineration',\n",
    "    '- disposal incineration': 'Disposal-Incineration',\n",
    "    'disposal incineration -': 'Disposal-Incineration',\n",
    "    'disposal-landfill': 'Disposal-Landfill',\n",
    "    'landfill - disposal': 'Disposal-Landfill',\n",
    "    'disposal - landfill': 'Disposal-Landfill',\n",
    "    '- landfill disposal': 'Disposal-Landfill',\n",
    "    'landfill disposal -': 'Disposal-Landfill',\n",
    "    'd i s p o s a l  - l a n d f i l l': 'Disposal-Landfill',\n",
    "    '- disposal landfill': 'Disposal-Landfill',\n",
    "    'disposal landfill -': 'Disposal-Landfill',\n",
    "    'recovery - recycling': 'Recovery-Recycling',\n",
    "    'recovery-recycling': 'Recovery-Recycling',\n",
    "    '- recovery recycling': 'Recovery-Recycling',\n",
    "    'r e c o v e r y  - r e c y c l i n g': 'Recovery-Recycling',\n",
    "    'recycling - recovery': 'Recovery-Recycling',\n",
    "    '- recycling recovery': 'Recovery-Recycling',\n",
    "    'recovery recycling -': 'Recovery-Recycling',\n",
    "    'recycling recovery -': 'Recovery-Recycling',\n",
    "    'recovery - composting': 'Recovery-Composting',\n",
    "    '- composting recovery': 'Recovery-Composting',\n",
    "    'r e c o v e r y  - c o m p o s t i n g': 'Recovery-Composting',\n",
    "    'recovery-composting': 'Recovery-Composting',\n",
    "    'composting - recovery': 'Recovery-Composting',\n",
    "    'recovery composting -': 'Recovery-Composting',\n",
    "    '- recovery composting': 'Recovery-Composting',\n",
    "    'composting recovery -': 'Recovery-Composting',\n",
    "    'recovery - energy recovery': 'Recovery-Energy Recovery',\n",
    "    'recovery energy recovery -': 'Recovery-Energy Recovery',\n",
    "    'r e c o v e r y  - e n e r g y  r e c o v e r y': 'Recovery-Energy Recovery',\n",
    "    'recovery - recovery energy': 'Recovery-Energy Recovery',\n",
    "    'recovery recovery energy -': 'Recovery-Energy Recovery',\n",
    "    '- energy recovery recovery': 'Recovery-Energy Recovery',\n",
    "    '- recovery recovery energy': 'Recovery-Energy Recovery',\n",
    "    '- recovery energy recovery': 'Recovery-Energy Recovery',\n",
    "    'recovery recovery - energy': 'Recovery-Energy Recovery',\n",
    "    'energy recovery recovery -': 'Recovery-Energy Recovery',\n",
    "    'energy - recovery recovery': 'Recovery-Energy Recovery',\n",
    "    'energy recovery - recovery': 'Recovery-Energy Recovery',\n",
    "    'recovery-energyrecovery': 'Recovery-Energy Recovery',\n",
    "    'recovery energy - recovery': 'Recovery-Energy Recovery',\n",
    "np.nan: np.nan,  # Keep NaN values as NaN\n",
    "    'nan': np.nan,\n",
    "    'n a n': np.nan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d98d7bc-4d6c-49e4-a3a1-a5540fd9aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data['treatment_method'] = merge_data['treatment_method'].replace(treatment_mapping)\n",
    "merge_data.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58164a6a-36f1-487a-871b-1ad2befe047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_entries=merge_data['treatment_method'].unique()\n",
    "for entry in unique_entries:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eaa05f-99cb-4489-bd90-fa2e1f6fdf9f",
   "metadata": {},
   "source": [
    "(Pandas.Series.value_counts — Pandas 1.3.4 Documentation, n.d.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c4679-d2b4-49c1-8799-54a55935e0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "value=merge_data['treatment_method'].value_counts()\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3198e277-d89a-44e9-bada-2af27e98eef9",
   "metadata": {},
   "source": [
    "\n",
    "- Converted `price_of_treatment` to numeric formats using a custom function `convert_to_float` to clean the column.\n",
    "\n",
    "- Removed non-numeric characters (e.g., commas, € signs)\n",
    "  \n",
    "### Outcome\n",
    "- Uniform text values across categories for consistent grouping which will make analysis easier.\n",
    "- Accurate numeric values for analysis.\n",
    "- Removed redundancy in text format.\n",
    "\n",
    "(Pandas.DataFrame.round — Pandas 1.3.4 Documentation, n.d.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e473ee-b9e3-41ce-8110-563d8368ee3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract and convert price to float\n",
    "def convert_to_float(value):\n",
    "    if isinstance(value, str):  # Process only if the value is a string\n",
    "        # Extract numeric value from the string\n",
    "        numeric_value = re.findall(r'[\\d\\.,]+', value)\n",
    "        \n",
    "        if numeric_value:\n",
    "            # Clean the numeric string (remove commas and extra spaces)\n",
    "            cleaned_value = numeric_value[0].replace(',', '').replace('€', '').strip()\n",
    "            \n",
    "            # Convert the cleaned string to float\n",
    "            return float(cleaned_value)\n",
    "        else:\n",
    "            return None  # If no valid numeric value is found\n",
    "    elif isinstance(value, (int, float)):  # If it's already numeric, return it as is\n",
    "        return value\n",
    "    else:\n",
    "        return None  # Handle any other unexpected types\n",
    "\n",
    "# Apply the function to the \"price_of_treatment\" column\n",
    "merge_data['price_of_treatment'] = merge_data['price_of_treatment'].apply(convert_to_float)\n",
    "\n",
    "merge_data['price_of_treatment'] = merge_data['price_of_treatment'].round(2)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(merge_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafb210f-dc07-4f2b-b65f-ba18a9c56aa1",
   "metadata": {},
   "source": [
    "# 3.Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc607ffb-b07c-4a32-bdfe-9fc8e3c9b68f",
   "metadata": {},
   "source": [
    "### A. Identifing columns with Missing Values:\n",
    "1. **treatment_method**: 21,950 missing values.\n",
    "2. **price_of _reatment**: 19,656 missing values.\n",
    "3. **waste_amount_in_tonnes**: 3,277 missing values.\n",
    "4. **Quarter**: 3,342 missing values.\n",
    "5. **county**: 3,269 missing values.\n",
    "6. **waste_type**: 3,312 missing values.\n",
    "7. **waste_category**: 3,242 missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a63e5f-94be-445d-b166-63bb7047f400",
   "metadata": {},
   "source": [
    "(Pandas DataFrame Isnull() Method, n.d.)  \n",
    "(Pandas DataFrame Sum() Method, n.d.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205455ec-0351-420d-a331-7611e380f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4152307b-a8f8-4521-8de1-a72841fa21c8",
   "metadata": {},
   "source": [
    "### B. Strategies Adopted to deal with the Missing Values:\n",
    "- **Forward Fill**: Filled missing value in categorical fields like `waste_category` and  `Quarter` using `ffill`.\n",
    "- **Forward Fill** and **Backward Fill**: Filled the missing values in  `treatment_method` with both methods because there were missing values present in the first and last row.\n",
    "  \n",
    "- **Imputation:**\n",
    "  - Median for numerical fields (e.g., `price_of_treatment`and `waste_amount_in_tonnes`) to prevent influence from extreme values.\n",
    " \n",
    "\n",
    "(Pandas DataFrame Fillna() Method, n.d.)  \n",
    "(Python Statistics.median() Method, n.d.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d978172e-3720-4e8c-8c1e-47c2d49e0c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data['Quarter'] = merge_data['Quarter'].fillna(method='ffill')  # Forward fill\n",
    "merge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e0d23-fa11-4d3f-81d8-d890612bf2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data['county'] = merge_data['county'].fillna(method='ffill')\n",
    "merge_data['waste_type'] = merge_data['waste_type'].fillna(method='ffill')\n",
    "merge_data['waste_category'] = merge_data['waste_category'].fillna(method='ffill')\n",
    "merge_data['treatment_method'] = merge_data['treatment_method'].fillna(method='ffill')\n",
    "merge_data['treatment_method'] = merge_data['treatment_method'].fillna(method='bfill')\n",
    "merge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c44ff8-219b-4440-b2d9-d01a33f80f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data['waste_amount_in_tonnes'] = merge_data['waste_amount_in_tonnes'].fillna(merge_data['waste_amount_in_tonnes'].median())\n",
    "merge_data['price_of_treatment'] = merge_data['price_of_treatment'].fillna(merge_data['price_of_treatment'].median())\n",
    "merge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a56535-a662-44cf-bb39-971584348b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3734092e-6736-4db2-b449-b407284d6df2",
   "metadata": {},
   "source": [
    "### C. Removing Duplicates  \n",
    "- Removing the duplicate rows since they might affect analysis accuracy by inflating the importance of redundant data.\n",
    "- Also removing duplicate rows will ensure reliability of the dataset and make better visualizations.\n",
    "\n",
    "(Pandas DataFrame Duplicated() Method | Pandas Method, 2018) \n",
    "(Python | Pandas Dataframe.drop_duplicates(), 2018)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb09900-030c-46a9-8b29-2df8cdac2b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge_data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ac672-6388-4e03-9647-962749edc047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows from the dataset\n",
    "merge_data= merge_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f05231-5dca-4dd6-b7a1-6cf5cefad173",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge_data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d13e44a-8bfb-4461-94ba-306369b887e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data['waste_amount_in_tonnes'].describe   #Calculating the mean before handling outliers to understand initial average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f394c090-da73-44e5-9f55-aa2ce7fde9b2",
   "metadata": {},
   "source": [
    "# 4. Detecting Outliers in the Dataset\n",
    " Data points that substantially differ from the rest of the observations in a dataset are known as outliers. \n",
    " Finding outliers is essential to preserving the integrity of the data and creating reliable models.\n",
    "\n",
    " ---\n",
    " \n",
    "## A. Finding Outliers for Waste Amount in Tonnes\n",
    "\n",
    "The following steps were used to identify outliers in the `waste_amount_in_tonnes` data.\n",
    "\n",
    "---\n",
    "\n",
    "### Steps to Identify Outliers\n",
    "\n",
    "1. **Calculate Quartiles (Q1 and Q3)**:  \n",
    "   The 25th percentile (Q1) and 75th percentile (Q3) were computed for the `waste_amount_in_tonnes` column.\n",
    "   \n",
    "2. **Calculate IQR**:  \n",
    "   The **Interquartile Range (IQR)** was determined as the difference between Q3 and Q1.\n",
    "   \n",
    "3. **Determine Outlier Bounds**:  \n",
    "   The outlier bounds were defined as:\n",
    "   - **Lower Bound**: \\( Q1 - 1.5 \\times IQR \\)\n",
    "   - **Upper Bound**: \\( Q3 + 1.5 \\times IQR \\)\n",
    "   \n",
    "4. **Identify Outliers**:  \n",
    "   The outliers were identified as values below the lower bound or above the upper bound.\n",
    "\n",
    "5. **Visualized Outliers**:  \n",
    "   The outliers in the `waste_amount_in_tonnes` data was identified and displayed by using boxplot and histogram.\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "(Pandas.DataFrame.quantile — Pandas 2.1.1 Documentation, n.d.)  \n",
    "(Waskom, n.d.)  \n",
    "(Matplotlib.pyplot.hist — Matplotlib 3.5.1 Documentation, n.d.)  \n",
    "(GeeksforGeeks, 2021)  \n",
    "(Zach, 2020) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6d71c1-de22-4bc6-bbee-3f6c964cdee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding outliers for waste amount in tonnes\n",
    "# Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "Q1 = merge_data['waste_amount_in_tonnes'].quantile(0.25)\n",
    "Q3 = merge_data['waste_amount_in_tonnes'].quantile(0.75)\n",
    "\n",
    "# Calculate IQR\n",
    "IQR_1 = Q3 - Q1\n",
    "\n",
    "# Determine outlier bounds\n",
    "lower_bound_1 = Q1 - 1.5 * IQR_1\n",
    "upper_bound_1 = Q3 + 1.5 * IQR_1\n",
    "\n",
    "# Identify outliers\n",
    "outliers_1 = merge_data[(merge_data['waste_amount_in_tonnes'] <= lower_bound_1) | (merge_data['waste_amount_in_tonnes'] >= upper_bound_1)]\n",
    "\n",
    "# Print outliers\n",
    "print(\"Identified Outliers:\")\n",
    "print(outliers_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff4d25f-2cc1-4bda-9107-1037d97fab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#boxplot\n",
    "sns.boxplot(data=merge_data, x='waste_amount_in_tonnes')\n",
    "plt.title('Boxplot for Identifying Outliers')\n",
    "plt.xlabel('outliers_1')\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(merge_data['waste_amount_in_tonnes'], bins=10, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram Before Handling Outliers')\n",
    "plt.xlabel('outliers_1')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fca0dc9-082d-4df4-b627-7f1193c3b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data to exclude outliers based on specified bounds\n",
    "merge_data = merge_data[(merge_data['waste_amount_in_tonnes'] >= lower_bound_1) & (merge_data['waste_amount_in_tonnes'] <= upper_bound_1)]\n",
    "merge_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44155108-2ced-41b7-bc13-7f68bfd16c41",
   "metadata": {},
   "source": [
    "(Pandas.DataFrame.describe — Pandas 0.20.2 Documentation, n.d.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8083e35-f33b-4750-808c-0de2022f1009",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data['waste_amount_in_tonnes'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abdf804-a611-473e-a786-92ff8695a1fe",
   "metadata": {},
   "source": [
    "## B. Visualizations  \n",
    " \n",
    "\n",
    "\n",
    "### Box Plot and Histogram for Waste Amount in Tonnes After Handling Outliers\n",
    "\n",
    "Below are the visualizations showing the `waste_amount_in_tonnes` data after addressing outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### Box Plot\n",
    "\n",
    "The box plot displays the distribution of the waste amounts, indicating if any outliers remain after handling them. It visually shows the data's spread and identifies outliers as points outside the whiskers.\n",
    "\n",
    "---\n",
    "\n",
    "### Histogram\n",
    "\n",
    "The histogram illustrates the frequency distribution of waste amounts, helping us understand how the data is distributed after handling outliers. It shows the number of occurrences within each bin and gives insights into the data's spread and central tendency.\n",
    "\n",
    "---\n",
    "\n",
    "These visualizations confirm that outliers have been addressed and provide a clearer view of the data's distribution. \n",
    "\n",
    "(Waskom, n.d.)  \r",
    "(Matplotlib.pyplot.hist — Matplotlib 3.5.1 Documentation, n.d.)l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677151ee-c096-4966-a711-73744e9df623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the boxplot\n",
    "sns.boxplot(data=merge_data, x='waste_amount_in_tonnes')\n",
    "plt.title('Boxplot after fixing Outliers')\n",
    "plt.xlabel('outliers_1')\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(merge_data['waste_amount_in_tonnes'], bins=10, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram after Handling Outliers')\n",
    "plt.xlabel('outliers_1')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "merge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbfd5da-afb2-41b5-bbd9-e091a5dd115c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data['price_of_treatment'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae403db2-10ec-481e-9351-1f337787955c",
   "metadata": {},
   "source": [
    "## Finding Outliers for Price of Treatment\n",
    "\n",
    "The following steps were used to identify outliers in the `price_of_treatment` data.\n",
    "\n",
    "---\n",
    "\n",
    "## A. Steps to Identify Outliers\n",
    "\n",
    "1. **Calculate Quartiles (Q1 and Q3)**:  \n",
    "   The 25th percentile (Q1) and 75th percentile (Q3) were computed for the `price_of_treatment` column.\n",
    "   \n",
    "2. **Calculate IQR**:  \n",
    "   The **Interquartile Range (IQR)** was determined as the difference between Q3 and Q1.\n",
    "   \n",
    "3. **Determine Outlier Bounds**:  \n",
    "   The outlier bounds were defined as:\n",
    "   - **Lower Bound**: \\( Q1 - 1.5 \\times IQR \\)\n",
    "   - **Upper Bound**: \\( Q3 + 1.5 \\times IQR \\)\n",
    "   \n",
    "4. **Identify Outliers**:  \n",
    "   The outliers were identified as values below the lower bound or above the upper bound.\n",
    "\n",
    "5. **Print Identified Outliers**:  \n",
    "   The outliers in the `price_of_treatment` data were displayed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefc404b-9881-4e61-9458-5fdb0dd586b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding outliers for price of treatment\n",
    "# Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "Qt1 = merge_data['price_of_treatment'].quantile(0.25)\n",
    "Qt3 = merge_data['price_of_treatment'].quantile(0.75)\n",
    "\n",
    "# Calculate IQR\n",
    "IQR_2 = Qt3 - Qt1\n",
    "\n",
    "# Determine outlier bounds\n",
    "lower_bound_2 = Qt1 - 1.5 * IQR_2\n",
    "upper_bound_2 = Qt3 + 1.5 * IQR_2\n",
    "\n",
    "# Identify outliers\n",
    "outliers_2 = merge_data[(merge_data['price_of_treatment'] <= lower_bound_2) | (merge_data['price_of_treatment'] >= upper_bound_2)]\n",
    "\n",
    "# Print outliers\n",
    "print(\"Identified Outliers:\")\n",
    "print(outliers_2)\n",
    "sns.boxplot(data=merge_data, x='price_of_treatment')\n",
    "plt.title('Boxplot for Identifying Outliers')\n",
    "plt.xlabel('outliers_2')\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(merge_data['price_of_treatment'], bins=10, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram before Handling Outliers')\n",
    "plt.xlabel('outliers_2')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2635f839-8afe-4d71-8eda-fcdbf27e34dd",
   "metadata": {},
   "source": [
    "(Pandas.DataFrame.clip — Pandas 2.1.4 Documentation, n.d.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47937db6-6238-49b3-9358-c1907cf095d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clip price values to be within the specified bounds\n",
    "merge_data['price_of_treatment'] = merge_data['price_of_treatment'].clip(lower=lower_bound_2, upper=upper_bound_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ede9f4-a281-4d58-a0db-c438d9b47f40",
   "metadata": {},
   "source": [
    "## B. Visualizations  \n",
    " \n",
    "\n",
    "\n",
    "### Box Plot and Histogram for Price of Treatment After Handling Outliers\n",
    "\n",
    "Below are the visualizations showing the `price_of_treatment` data after addressing outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### Box Plot\n",
    "\n",
    "The box plot below visualizes the `price_of_treatment` column and highlights any outliers:\n",
    "\n",
    "---\n",
    "\n",
    "### Histogram\n",
    "\n",
    "The histogram shows the frequency distribution of `price_of_treatment` before handling outliers, helping in understanding how the data is distributed.\n",
    "\n",
    "These visualizations help confirm the identification of outliers and provide insight into the data's distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5994e0ea-2f6e-4fb2-ab47-4ad8711b45d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the boxplot\n",
    "sns.boxplot(data=merge_data, x='price_of_treatment')\n",
    "plt.title('Boxplot after fixing Outliers')\n",
    "plt.xlabel('outliers_2')\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram\n",
    "plt.hist(merge_data['price_of_treatment'], bins=10, color='skyblue', edgecolor='black')\n",
    "plt.title('Histogram after Handling Outliers')\n",
    "plt.xlabel('outliers_2')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "merge_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7318e15-def7-43f8-b2b3-d4f7002c6388",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data['price_of_treatment'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd042de-2568-409e-a9aa-1e98687108d9",
   "metadata": {},
   "source": [
    "The data before and after treating outliers is clearly understood with these visuals. The histogram displays the better distribution following outlier treatment, while the box plot illustrates the existence and magnitude of outliers in the `waste_amount_in_tonnes` and `price_of_treatment` data. Resolving outliers guarantees a more balanced dataset, lowering skewness and facilitating more precise and trustworthy analysis for making decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af0ec73-8808-467f-bf2b-3fed3e05e86f",
   "metadata": {},
   "source": [
    "## 5.Description of the calculated fields.  \n",
    "\n",
    "The **Total Treatment Cost** and **Treatment Efficiency** computed fields were created to help organize and interpret the data. These measures provide a clearer picture of the data trends and comparisons, which streamlined the study. Various visualizations were used to find patterns, identify abnormalities, and comprehend linkages within the data during the **Exploratory Data Analysis (EDA)**. Bar charts made cross-category comparisons simple, box plots revealed outliers, and histograms offered information about the distribution of the data. The integrity of the data was preserved by using these visual aids and computations, and significant conclusions were drawn to help the project's objectives. perceptive analysis.  \n",
    "\n",
    "### Total Treatment Cost Calculation\n",
    "Large amounts are made easier to handle and understand by calculating the total treatment cost in millions. `waste_amount_in_tonnes` is multiplied by `price_of_treatment` and then divided by 1,000,000 to convert the cost into millions, which facilitates the analysis and interpretation of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d39ead-1e9f-4f83-9533-7f05ed8bd9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total_treatment_cost is in millions\n",
    "merge_data['total_treat_cost_millions'] =( merge_data['waste_amount_in_tonnes'] * merge_data['price_of_treatment'])/1000000\n",
    "merge_data['total_treat_cost_millions']=merge_data['total_treat_cost_millions'].round(2)\n",
    "merge_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb8505-1f30-494c-8251-f66182fc4182",
   "metadata": {},
   "source": [
    "### Treatment Efficiency Calculation\n",
    "The percentage of `waste_amount_in_tonnes` is rounded to two decimal places and compared to the dataset's maximum value is the treatment efficiency.\n",
    "(Pandas.DataFrame — Pandas 0.25.3 Documentation, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da41688b-b8b0-4d84-a067-83cb338d3274",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_data['treatment_efficiency'] = (merge_data['waste_amount_in_tonnes'] / merge_data['waste_amount_in_tonnes'].max()) * 100\n",
    "merge_data['treatment_efficiency']=merge_data['treatment_efficiency'].round(2)\n",
    "merge_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c72df5-de5b-4fc0-a209-835b73a093b6",
   "metadata": {},
   "source": [
    "## 6. Exploring the data visually using various graphs.\n",
    "\n",
    "### A. Quarterly Waste Amount Trends by Waste Type\n",
    "The trends in waste amounts throughout various quarters, broken down by waste kind, are shown in this line plot. The data is displayed using **sns.lineplot**, with lines colored according to waste type for convenient comparison. A title, axis labels, and a legend placed outside for clarification are all part of the plot. The layout is modified to accommodate the plot elements, and the X-axis labels are rotated for improved readability.\n",
    "(Seaborn.lineplot — Seaborn 0.11.2 Documentation, n.d.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972004ce-cfa0-457f-9a5d-1a9c7b201e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot waste trends by quarter and type\n",
    "sns.lineplot(data=merge_data, x='Quarter', y='waste_amount_in_tonnes', hue='waste_type', errorbar=None)\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Quarterly Waste Amount Trends by Waste Type', fontsize=16)\n",
    "plt.xlabel('Quarter', fontsize=12)\n",
    "plt.ylabel('Waste Amount (Tonnes)', fontsize=12)\n",
    "\n",
    "# Adjust legend and ticks\n",
    "plt.legend(title='Waste Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Fit layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c022a-fe11-42f7-a465-7e748e7946d9",
   "metadata": {},
   "source": [
    "Hazardous waste consistently exceeds non-hazardous waste, fluctuating around 9,500–10,500 tonnes, while non-hazardous waste ranges from 8,000–8,500 tonnes. Both types show irregular patterns without a clear trend, likely due to seasonal or operational factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d586fa-d8df-4b1e-9f17-b39bc88b9c1f",
   "metadata": {},
   "source": [
    "### B. Waste Amount by Category and Type\n",
    "\n",
    "This bar plot visualizes the waste amounts by category, differentiated by waste type. It uses **sns.barplot** to display the data, with the bars representing waste amounts for each category, and the legend indicating the waste types. The x-axis labels are rotated for readability, and the layout is adjusted for better spacing. \n",
    "(Seaborn.barplot — Seaborn 0.11.1 Documentation, n.d.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc7279f-487f-4868-8e1d-eb209328b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size for the plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Create a barplot showing waste amount by category and type\n",
    "sns.barplot(data=merge_data, x='waste_category', y='waste_amount_in_tonnes', hue='waste_type', errorbar=None)\n",
    "\n",
    "# Set the title for the plot\n",
    "plt.title('Waste Amount by Category and Type', fontsize=16)\n",
    "\n",
    "# Label the x-axis and y-axis\n",
    "plt.xlabel('Waste Category', fontsize=12)\n",
    "plt.ylabel('Waste Amount (Tonnes)', fontsize=12)\n",
    "\n",
    "# Adjust the legend position\n",
    "plt.legend(title='Waste Type', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Ensure layout fits into the figure area\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b30ea3-6e22-44bd-a4ad-801d4bb76253",
   "metadata": {},
   "source": [
    "Hazardous waste dominates most categories, especially in chemical, industrial, and electronic waste, while non-hazardous waste closely matches in organic and construction waste. Overall, waste generation is balanced across both types with no drastic variations.\n",
    "has context menu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0596569d-ae06-4629-b619-10ecb830ebe7",
   "metadata": {},
   "source": [
    "### C. Top 10 Counties by Total Waste Amount\n",
    "\n",
    "The top ten counties with the highest overall trash amounts are displayed in this bar plot. The information is arranged in descending order and categorized by county. The plot has rotated x-axis labels for readability and a color scheme for aesthetic appeal. Better spacing is achieved by adjusting the layout. \n",
    "(Seaborn.barplot — Seaborn 0.11.1 Documentation, n.d.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5581a1-8ef2-4ead-b36a-aa7d8d637d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 10 counties by waste amount\n",
    "top_counties = merge_data.groupby('county')['waste_amount_in_tonnes'].sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "# Plot bar chart of top counties\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.barplot(x= top_counties.index, y=top_counties.values, palette='plasma',hue= top_counties, legend= False)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title('Top 10 Counties by Total Waste Amount', fontsize=16)\n",
    "plt.xlabel('County', fontsize=12)\n",
    "plt.ylabel('Total Waste Amount (Tonnes)', fontsize=12)\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7678ac56-ba6b-4cd3-957a-ddcb3bbf7c92",
   "metadata": {},
   "source": [
    "The graph shows a relatively consistent waste generation among the top counties, with Wicklow slightly ahead. No sharp spikes or drastic drops are observed, indicating a similar waste production pattern across these counties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7966ba-c0dc-48bb-831d-ba3332e49d7d",
   "metadata": {},
   "source": [
    "### D. Waste Distribution by Type\n",
    "\n",
    "The distribution of total trash by kind is depicted in this pie chart, which also displays the percentage of each type. The y-axis label is buried for improved aesthetics, and the chart employs a pastel color scheme for visual clarity.\n",
    "\n",
    "(Pandas.DataFrame.plot.pie — Pandas 1.3.2 Documentation, n.d.)  \n",
    "(seaborn, 2013)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23b6a9f-1ebb-4270-8505-9181bb8b3a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group and sum waste amounts by type\n",
    "waste_by_type = merge_data.groupby('waste_type')['waste_amount_in_tonnes'].sum()\n",
    "\n",
    "# Plot pie chart for waste distribution by type\n",
    "plt.figure(figsize=(8, 8))\n",
    "waste_by_type.plot(kind='pie', autopct='%1.1f%%', startangle=140, colors=sns.color_palette('Pastel1'))\n",
    "\n",
    "# Title, remove y-label, and adjust layout\n",
    "plt.title('Pie Chart: Waste Distribution by Type', fontsize=16)\n",
    "plt.ylabel('')  # Hide y-label for better visual\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0fc481-cdca-4dac-a56c-bf7c511f9761",
   "metadata": {},
   "source": [
    "The distribution of **Hazardous** and **Non-Hazardous** waste is depicted in the pie chart, which indicates that **42.7%** of the total waste is **Hazardous** and **57.3%** is **Non-Hazardous**. This suggests that most of the garbage handled in the dataset is less dangerous and may call for different management approaches because it shows a somewhat larger percentage of non-hazardous waste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5f4939-c4a3-4724-bae9-c591c0efa733",
   "metadata": {},
   "source": [
    "### E. Distribution of Waste Amounts\n",
    "Using a kernel density estimate (KDE) to visualize the general structure of the data, this histogram displays the distribution of waste quantities. It aids in comprehending how frequently various waste quantities occur within the dataset. \n",
    "\n",
    "(Seaborn.histplot — Seaborn 0.11.2 Documentation, n.d.)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0966e5-7e81-40e8-8113-fe6813487c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram with KDE for waste amount distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=merge_data, x='waste_amount_in_tonnes', bins=30, kde=True, color='orange')\n",
    "\n",
    "# Set title and labels\n",
    "plt.title(' Distribution of Waste Amounts', fontsize=16)\n",
    "plt.xlabel('Waste Amount (Tonnes)', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eba586-a860-46be-941c-29e7824b143b",
   "metadata": {},
   "source": [
    "With most of the data concentrated between 0 and 10,000 tonnes, this histogram shows a positively skewed distribution of waste amounts, suggesting that smaller waste amounts are more common. The steady drop toward higher values raises the possibility of extreme outliers or infrequent instances of significant waste volumes, both of which merit more research"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2018c85-221d-4d53-9083-df2d3137acae",
   "metadata": {},
   "source": [
    "# 7. Analysis of the findings based on EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ada0fe9-5d09-45ff-98b9-6ce7b7e173b8",
   "metadata": {},
   "source": [
    "The **Explanatory Data Analysis(EDA)** highlighted waste treatment trends, costs, and efficiency, simplifying financial analysis by expressing costs in millions and identifying improvement areas by comparing waste amounts to the dataset's maximum value. Key findings include: hazardous waste consistently exceeding non-hazardous waste in quarterly trends, with fluctuations likely due to seasonal factors; dominance of hazardous waste in categories like industrial and chemical waste, while non-hazardous waste is significant in organic and construction waste; and Wicklow leading in total waste generation, with consistent patterns across top counties. A pie chart further pinpointed major waste contributors, guiding targeted reduction initiatives and enhancing treatment efficiency.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac957618-2efc-414b-94b9-89dd32f7e645",
   "metadata": {},
   "source": [
    "# 8. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4dc40e-3c17-4b6b-8f92-7cd83e58c7b7",
   "metadata": {},
   "source": [
    "The **quarterly_waste_generation.csv** and **quarterly_waste_treatment.csv** waste-related datasets were successfully combined after significant errors including missing values and different date formats were fixed. We discovered noteworthy trends throughout the exploratory data analysis (EDA), including major differences in garbage creation between counties, with some producing far more rubbish than others. Furthermore, we saw that the expenses related to waste treatment differed based on the treatment technique employed, underscoring the possible influence of various treatment approaches on total expenses. In order to better understand the factors influencing treatment costs, we plan to investigate prediction models in the future. This analysis offered insightful information about waste management techniques. By doing this, we intend to better understand the factors influencing waste management results and determine the most economical waste treatment techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ec60df-33c3-49a4-b2c0-b87d5c94c472",
   "metadata": {},
   "source": [
    "# 9. Reflections and Insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e139bf-7c8e-4285-9893-3ee6cd5a84f2",
   "metadata": {},
   "source": [
    "## A. Lessons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c942d5a1-d4fe-4ad2-903f-7f57ad43f000",
   "metadata": {},
   "source": [
    "## Group Member 1: Dyuti Kamdar\n",
    "**Most enjoyable part:** I enjoyed exploring Python's data visualization libraries like seaborn and creating insightful graphs using it. It was fun to learn the plotting of bar graphs and histogram.  \n",
    "**Most challenging aspect:** The most challenging part for me was handling and cleaning a large dataset especially waste category and treatment method columns which had a lot of missing and all inconsistent values.  \n",
    "**Most valuable thing learned:** I learned the importance of thorough data cleaning and preprocessing data for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad52fa-7790-42c6-92d1-64468757aea4",
   "metadata": {},
   "source": [
    "## Group Member 2: Mrunal Howale\n",
    "**Most enjoyable part:** Finding trends in waste treatment across various categories and time periods through data analysis, along with automating repetitive tasks using Python, was the most enjoyable part of the project.  \n",
    "**Most challenging aspect:**  The most challenging aspect was cleaning the dataset of quarterly waste treatment, which involved addressing inconsistencies and missing values, along with struggling to understand and implement advanced statistical methods in Python for data analysis.  \n",
    "**Most valuable thing learned:** The most valuable thing learned through this project was the importance of effective data preparation and visualization for drawing meaningful insights, along with gaining a solid understanding of using pandas and NumPy for data manipulation and the power of Python for data analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd34aede-576a-4588-99e5-57686a28f152",
   "metadata": {},
   "source": [
    "## Group Member 3: Praveen Mangawa\n",
    "**Most enjoyable part:** I loved working with Python libraries like Matplotlib and Seaborn to visually communicate data trends and insights.  \n",
    "**Most challenging aspect:**  The biggest challenge for me was debugging code errors and ensuring the accuracy of calculations in the project.  \n",
    "**Most valuable thing learned:** The most valuable thing that I learned while working on this project is what not to do while coding in python and also learned coding ethics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fdafc2-33d2-4510-96f0-2d892a054b9f",
   "metadata": {},
   "source": [
    "## Group Member 4: Shakirat Muibi\n",
    "**Most enjoyable part:** I gained satisfaction from exploring and cleaning an inconsistent dataset, as well as handling missing values, appreciating how small corrections enhanced analysis insights.  \n",
    "**Most challenging aspect:** I found normalizing inconsistent date and text formats challenging, as it required time and meticulous effort.\n",
    "   \n",
    "**Most valuable thing learned:** Learned essential skills in cleaning, merging datasets, and handling missing data and outliers, crucial for tackling real-world data complexities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f01d91-43d4-4499-9096-59cce349c7bc",
   "metadata": {},
   "source": [
    "## Group Member 5: Aida George\n",
    "**Most enjoyable part:** I loved creating interactive Jupyter Notebooks to showcase the project results in an engaging and professional format.  \n",
    "**Most challenging aspect:**  The most challenging aspect was understanding and implementing the logic for outlier detection and correction.  \n",
    "**Most valuable thing learned:** I learned how to recognize and manage outliers using the IQR technique, which can greatly influence the conclusions derived from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9079e1e3-5429-4ad3-be3f-a631849480b2",
   "metadata": {},
   "source": [
    "## Group Member 6: Dion Chettiar\n",
    "**Most enjoyable part:**  I enjoyed learning how to merge and transform datasets into meaningful formats, which brought clarity to the data analysis process.  \n",
    "**Most challenging aspect:** I found working with string manipulation and correcting inconsistent entries in the dataset particularly difficult.  \n",
    "**Most valuable thing learned:** I learned the importance of data normalization and how to use Python's re and pandas libraries to handle text-based inconsistencies effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824da08a-5e7b-4402-a54c-7120db88dd0d",
   "metadata": {},
   "source": [
    "## B. Issues Encountered during the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249dbb6b-1d81-4abf-876b-6863eaaf4ca3",
   "metadata": {},
   "source": [
    "We ran into a number of problems when cleansing the data. Managing missing entries in the ` treatment_method`  column, where the initial value was blank, was one major obstacle. In order to rectify the issue of advance filling producing a count of one instead of zero, we also employed backfilling in conjunction with forward filling. When dates were kept as strings and expenses were specified in various units (such as thousands and millions), another problem surfaced during data transformation. In order to fix this, we transformed the dates into the appropriate format and standardized all values to a common unit (millions), guaranteeing uniformity and facilitating precise analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8371e102-a6aa-45a3-ace6-994f6a95d9e3",
   "metadata": {},
   "source": [
    "# 10. References\n",
    "\n",
    "1.\tNumpy. (2024). NumPy. Numpy.org. https://numpy.org/\r\n",
    "\r\n",
    "2.\tPandas. (2018). Python Data Analysis Library. Pydata.org. https://pandas.pydata.org/\r\n",
    "\r\n",
    "3.\tseaborn. (2012). seaborn: statistical data visualization — seaborn 0.9.0 documentation. Pydata.org. https://seaborn.pydata.org/\r\n",
    "\r\n",
    "4.\tMatplotlib. (2012). Matplotlib: Python plotting — Matplotlib 3.1.1 documentation. Matplotlib.org. https://matplotlib.org/\r\n",
    "\r\n",
    "5.\tPython Software Foundation. (2020). csv — CSV File Reading and Writing — Python 3.8.1 documentation. Python.org. https://docs.python.org/3/library/csv.html\r\n",
    "\r\n",
    "6.\tPython. (2009). re — Regular expression operations — Python 3.7.2 documentation. Python.org. https://docs.python.org/3/library/re.html\r\n",
    "\r\n",
    "7.\tpandas.merge — pandas 1.2.3 documentation. Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.merge.html\r\n",
    "\r\n",
    "8.\tpandas.DataFrame.rename — pandas 1.4.2 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html\r\n",
    "\r\n",
    "9.\tpandas.DataFrame.nunique — pandas 1.3.4 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.nunique.html\r\n",
    "\r\n",
    "10.\tpandas.to_datetime — pandas 1.3.4 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html\r\n",
    "\r\n",
    "11.\tpandas.NaT — pandas 2.2.3 documentation. (2024). Pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.NaT.html\r\n",
    "\r\n",
    "12.\tGeeksforGeeks. (2018, July 3). Python | Pandas.apply(). GeeksforGeeks; GeeksforGeeks. https://www.geeksforgeeks.org/python-pandas-apply/\r\n",
    "\r\n",
    "13.\tpandas.Series.str.replace — pandas 2.0.3 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.Series.str.replace.html\r\n",
    "\r\n",
    "14.\tpandas.Series.str.strip — pandas 2.0.0 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.Series.str.strip.html\r\n",
    "\r\n",
    "15.\tpandas.DataFrame.replace — pandas 1.2.4 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\r\n",
    "\r\n",
    "16.\tWorking with missing data — pandas 1.5.1 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/user_guide/missing_data.html\r\n",
    "\r\n",
    "17.\tpandas.DataFrame.map — pandas 2.2.2 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.map.html\r\n",
    "\r\n",
    "18.\tpandas.to_numeric — pandas 1.4.2 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.to_numeric.html\r\n",
    "\r\n",
    "19.\tpandas.Series.str.replace — pandas 2.0.3 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.Series.str.replace.html\r\n",
    "\r\n",
    "20.\tpandas.Series.str.lower — pandas 2.2.2 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.Series.str.lower.html\r\n",
    "\r\n",
    "21.\tpandas.Series.value_counts — pandas 1.3.4 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html\r\n",
    "\r\n",
    "22.\tpandas.DataFrame.round — pandas 1.3.4 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.round.html\r\n",
    "\r\n",
    "23.\tPandas DataFrame isnull() Method. (n.d.). www.w3schools.com. https://www.w3schools.com/python/pandas/ref_df_isnull.asp\r\n",
    "\r\n",
    "24.\tPandas DataFrame sum() Method. (n.d.). www.w3schools.com. https://www.w3schools.com/python/pandas/ref_df_sum.asp\r\n",
    "\r\n",
    "25.\tPandas DataFrame fillna() Method. (n.d.). Www.w3schools.com. https://www.w3schools.com/python/pandas/ref_df_fillna.asp\r\n",
    "\r\n",
    "26.\tPython statistics.median() Method. (n.d.). Www.w3schools.com. https://www.w3schools.com/python/ref_stat_median.asp\r\n",
    "\r\n",
    "27.\tPandas DataFrame duplicated() Method | Pandas Method. (2018, July 23). GeeksforGeeks. https://www.geeksforgeeks.org/pandas-dataframe-duplicated/\r\n",
    "\r\n",
    "28.\tPython | Pandas dataframe.drop_duplicates(). (2018, August 2). GeeksforGeeks. https://www.geeksforgeeks.org/python-pandas-dataframe-drop_duplicates/\r\n",
    "\r\n",
    "29.\tpandas.DataFrame.quantile — pandas 2.1.1 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.quantile.html\r\n",
    "\r\n",
    "30.\tWaskom, M. (n.d.). seaborn.boxplot — seaborn 0.11.1 documentation. Seaborn.pydata.org. https://seaborn.pydata.org/generated/seaborn.boxplot.html\r\n",
    "\r\n",
    "31.\tmatplotlib.pyplot.hist — Matplotlib 3.5.1 documentation. (n.d.). Matplotlib.org. https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\r\n",
    "\r\n",
    "32.\tGeeksforGeeks. (2021, February 23). Detect and Remove the Outliers using Python. GeeksforGeeks. https://www.geeksforgeeks.org/detect-and-remove-the-outliers-using-python/\r\n",
    "\r\n",
    "33.\tZach. (2020, July 6). How to Remove Outliers in Python. Statology. https://www.statology.org/remove-outliers-python/\r\n",
    "\r\n",
    "34.\tpandas.DataFrame.describe — pandas 0.20.2 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/pandas-docs/version/0.20.2/generated/pandas.DataFrame.describe.html\r\n",
    "\r\n",
    "35.\tWaskom, M. (n.d.). seaborn.boxplot — seaborn 0.11.1 documentation. Seaborn.pydata.org. https://seaborn.pydata.org/generated/seaborn.boxplot.html\r\n",
    "\r\n",
    "36.\tmatplotlib.pyplot.hist — Matplotlib 3.5.1 documentation. (n.d.). Matplotlib.org. https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html\r\n",
    "\r\n",
    "37.\tpandas.DataFrame.clip — pandas 2.1.4 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.clip.html\r\n",
    "\r\n",
    "38.\tpandas.DataFrame — pandas 0.25.3 documentation. (2020). Pydata.org. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\r\n",
    "\r\n",
    "39.\tseaborn.lineplot — seaborn 0.11.2 documentation. (n.d.). Seaborn.pydata.org. https://seaborn.pydata.org/generated/seaborn.lineplot.html\r\n",
    "\r\n",
    "40.\tseaborn.barplot — seaborn 0.11.1 documentation. (n.d.). Seaborn.pydata.org. https://seaborn.pydata.org/generated/seaborn.barplot.html\r\n",
    "\r\n",
    "41.\tpandas.DataFrame.plot.pie — pandas 1.3.2 documentation. (n.d.). Pandas.pydata.org. https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.plot.pie.html\r\n",
    "\r\n",
    "42.\tseaborn. (2013). Choosing color palettes — seaborn 0.9.0 documentation. Pydata.org. https://seaborn.pydata.org/tutorial/color_palettes.html\r\n",
    "\r\n",
    "43.\tseaborn.histplot — seaborn 0.11.2 documentation. (n.d.). Seaborn.pydata.org. https://seaborn.pydata.org/generated/seaborn.histplot.html\r\n",
    "\r\n",
    "44.\t‌ Grammarly. (2). Grammarly. Grammarly.com. [AI-based proofreading tool] https://www.grammarly.com\r\n",
    "\r\n",
    "orn.pydata.org. https://seaborn.pydata.org/generated/seaborn.histplot.html\r\n",
    "\r\n",
    "44.\t‌\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "‌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be81c46-7063-45dd-b662-16e0f23a385f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
